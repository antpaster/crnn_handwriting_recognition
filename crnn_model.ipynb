{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3305d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import string\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f115a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 1) Character set & CTC codec\n",
    "# ----------------------------\n",
    "def default_charset():\n",
    "    # You can customize this to match your data (e.g., only lowercase + space)\n",
    "    # Keep space ' ' included if your lines contain spaces.\n",
    "    charset = list(string.digits + string.ascii_letters + string.punctuation + ' ')\n",
    "    # Remove characters you know you don't have, or add accents if needed.\n",
    "    return charset\n",
    "\n",
    "class CTCCodec:\n",
    "    \"\"\"\n",
    "    Maps characters <-> indices. Index 0 is reserved for CTC blank.\n",
    "    \"\"\"\n",
    "    def __init__(self, charset: List[str]):\n",
    "        self.blank_idx = 0\n",
    "        self.chars = ['<BLK>'] + charset\n",
    "        self.char2idx = {c: i+1 for i, c in enumerate(charset)}  # shift by +1\n",
    "        self.idx2char = {i+1: c for i, c in enumerate(charset)}\n",
    "\n",
    "    def encode(self, text: str) -> torch.Tensor:\n",
    "        return torch.tensor([self.char2idx[c] for c in text if c in self.char2idx], dtype=torch.long)\n",
    "\n",
    "    def decode_greedy(self, logits: torch.Tensor) -> List[str]:\n",
    "        \"\"\"\n",
    "        logits: (T, N, C) log-probs or raw scores. We'll argmax over classes.\n",
    "        Returns list of length N with collapsed CTC decoding.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(dim=-1)  # (T, N)\n",
    "            pred = pred.cpu().numpy()\n",
    "        N = pred.shape[1]\n",
    "        texts = []\n",
    "        for n in range(N):\n",
    "            seq = pred[:, n]\n",
    "            prev = -1\n",
    "            out = []\n",
    "            for idx in seq:\n",
    "                if idx != self.blank_idx and idx != prev:\n",
    "                    out.append(self.idx2char.get(int(idx), ''))\n",
    "                prev = idx\n",
    "            texts.append(''.join(out))\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf5962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# 2) Image transforms & augmentations\n",
    "# ----------------------------------\n",
    "class KeepRatioResize:\n",
    "    \"\"\"\n",
    "    Resize PIL image to target height with proportional width, no crop.\n",
    "    \"\"\"\n",
    "    def __init__(self, target_h: int):\n",
    "        self.target_h = target_h\n",
    "\n",
    "    def __call__(self, img: Image.Image) -> Image.Image:\n",
    "        w, h = img.size\n",
    "        if h == self.target_h:\n",
    "            return img\n",
    "        new_w = max(1, round(w * (self.target_h / h)))\n",
    "        return img.resize((new_w, self.target_h), Image.BILINEAR)\n",
    "\n",
    "class ElasticLike:\n",
    "    \"\"\"\n",
    "    Lightweight 'elastic' style warp using PIL perspective + slight blur/sharpen.\n",
    "    Keeps text legible but varied.\n",
    "    \"\"\"\n",
    "    def __init__(self, p=0.5, max_warp=0.08):\n",
    "        self.p = p\n",
    "        self.max_warp = max_warp\n",
    "\n",
    "    def __call__(self, img: Image.Image) -> Image.Image:\n",
    "        if random.random() > self.p:\n",
    "            return img\n",
    "        w, h = img.size\n",
    "        dx = int(self.max_warp * w)\n",
    "        dy = int(self.max_warp * h)\n",
    "        # random offsets for corners\n",
    "        src = [(0,0),(w,0),(w,h),(0,h)]\n",
    "        img = img.transform((w + random.randint(-dx, dx), h + random.randint(-dx, dx)), Image.QUAD, src)\n",
    "        if random.random() < 0.5:\n",
    "            img = img.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.2, 0.6)))\n",
    "        if random.random() < 0.3:\n",
    "            img = img.filter(ImageFilter.UnsharpMask(radius=1.0, percent=80, threshold=3))\n",
    "        return img\n",
    "\n",
    "def pil_to_tensor_normalized(img: Image.Image) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert PIL (grayscale) -> Tensor in [0,1], normalize to mean=0.5, std=0.5\n",
    "    Output shape: (1, H, W)\n",
    "    \"\"\"\n",
    "    t = transforms.functional.pil_to_tensor(img).float() / 255.0  # (1,H,W) for 'L'\n",
    "    return transforms.functional.normalize(t, mean=[0.5], std=[0.5])\n",
    "\n",
    "def binarize_if_needed(img: Image.Image, p=0.0):\n",
    "    if p > 0 and random.random() < p:\n",
    "        return img.convert('L').point(lambda x: 255 if x > 200 else 0, mode='L')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37191bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# 3) Dataset definitions\n",
    "# ------------------------\n",
    "class LinesFile(Dataset):\n",
    "    \"\"\"\n",
    "    labels.txt format: path<TAB>text (UTF-8)\n",
    "    Converts to grayscale, resizes to H=64 with proportional width.\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path: str, codec: CTCCodec, target_h: int = 64, keep_aspect=True, binarize_p=0.0):\n",
    "        super().__init__()\n",
    "        self.samples = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.rstrip('\\n')\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                parts = line.split('\\t', 1)\n",
    "                if len(parts) != 2:\n",
    "                    continue\n",
    "                path, text = parts\n",
    "                self.samples.append((path, text))\n",
    "        self.codec = codec\n",
    "        self.target_h = target_h\n",
    "        self.keep_aspect = keep_aspect\n",
    "        self.resize = KeepRatioResize(target_h)\n",
    "        self.binarize_p = binarize_p\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, text = self.samples[idx]\n",
    "        img = Image.open(path).convert('L')\n",
    "        img = ImageOps.exif_transpose(img)\n",
    "        if self.keep_aspect:\n",
    "            img = self.resize(img)\n",
    "        img = binarize_if_needed(img, self.binarize_p)\n",
    "        tensor = pil_to_tensor_normalized(img)\n",
    "        label = self.codec.encode(text)\n",
    "        return tensor, label, text, os.path.basename(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602dba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedWrapper(Dataset):\n",
    "    \"\"\"\n",
    "    Wrap a base dataset and apply strong augmentations.\n",
    "    Use multiple instances of this wrapper to grow dataset size to >=5x.\n",
    "    \"\"\"\n",
    "    def __init__(self, base: LinesFile):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        # Compose handwriting-friendly augmentations\n",
    "        self.resize = KeepRatioResize(base.target_h)\n",
    "        self.aug = transforms.RandomChoice([\n",
    "            transforms.RandomAffine(degrees=2, translate=(0.02, 0.03), scale=(0.95, 1.05), shear=(-2, 2), fill=255),\n",
    "            transforms.RandomPerspective(distortion_scale=0.3, p=1.0),\n",
    "        ])\n",
    "        self.colorjitter = transforms.ColorJitter(brightness=0.2, contrast=0.2)\n",
    "        self.elastic = ElasticLike(p=0.7, max_warp=0.06)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tensor, label, text, name = self.base[idx]\n",
    "        # back to PIL to apply augmentations that expect PIL\n",
    "        H = tensor.shape[1]\n",
    "        W = tensor.shape[2]\n",
    "        pil = transforms.functional.to_pil_image(((tensor * 0.5 + 0.5) * 255.0).byte())  # unnormalize for aug\n",
    "        # augment in PIL space\n",
    "        pil = self.elastic(pil)\n",
    "        pil = self.aug(pil)\n",
    "        pil = self.colorjitter(pil)\n",
    "        # Small random Gaussian blur helps mimic scanning\n",
    "        if random.random() < 0.3:\n",
    "            pil = pil.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.2, 0.7)))\n",
    "        # Ensure size back to desired height (keeps ratio)\n",
    "        pil = self.resize(pil)\n",
    "        # Occasionally invert (handwriting scans vary)\n",
    "        if random.random() < 0.25:\n",
    "            pil = ImageOps.invert(pil)\n",
    "        tensor_aug = pil_to_tensor_normalized(pil)\n",
    "        return tensor_aug, label, text, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c50a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# 4) Collate: pad widths & build lengths\n",
    "# ---------------------------------------\n",
    "@dataclass\n",
    "class Batch:\n",
    "    imgs: torch.Tensor        # (B, 1, H, Wmax)\n",
    "    labels: torch.Tensor      # (sum_targets,)\n",
    "    label_lengths: torch.Tensor  # (B,)\n",
    "    input_lengths: torch.Tensor  # (B,)  number of time steps per sample after CNN\n",
    "    texts: List[str]\n",
    "    names: List[str]\n",
    "    orig_widths: List[int]\n",
    "\n",
    "class PadCollate:\n",
    "    \"\"\"\n",
    "    Pads each batch to max width (also to multiple of 4) and prepares CTC lengths.\n",
    "    \"\"\"\n",
    "    def __init__(self, multiple_of: int = 4, height: int = 64):\n",
    "        self.multiple_of = multiple_of\n",
    "        self.height = height\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # batch: list of (tensor(1,H,W), label, text, name)\n",
    "        imgs, labels, texts, names, widths = [], [], [], [], []\n",
    "        for t, lab, txt, name in batch:\n",
    "            _, h, w = t.size()\n",
    "            assert h == self.height, f\"Expected height {self.height}, got {h}\"\n",
    "            imgs.append(t)\n",
    "            labels.append(lab)\n",
    "            texts.append(txt)\n",
    "            names.append(name)\n",
    "            widths.append(w)\n",
    "\n",
    "        B = len(imgs)\n",
    "        max_w = max(widths)\n",
    "        # pad to next multiple of self.multiple_of (for CNN width downsampling)\n",
    "        if self.multiple_of > 1:\n",
    "            max_w = int(math.ceil(max_w / self.multiple_of) * self.multiple_of)\n",
    "\n",
    "        padded = torch.full((B, 1, self.height, max_w), fill_value=(0.5 - 0.5)/0.5, dtype=imgs[0].dtype)\n",
    "        # Explanation: because we normalized to mean=0.5, std=0.5,\n",
    "        # \"white\" (1.0) becomes (1-0.5)/0.5 = +1.0, \"gray 0.5\" is 0; but to avoid halo,\n",
    "        # we can pad with normalized value of 1.0 (white) -> +1.0:\n",
    "        padded.fill_(+1.0)\n",
    "\n",
    "        for i, t in enumerate(imgs):\n",
    "            _, _, w = t.size()\n",
    "            padded[i, :, :, :w] = t\n",
    "\n",
    "        labels_concat = torch.cat(labels, dim=0)\n",
    "        label_lengths = torch.tensor([len(l) for l in labels], dtype=torch.long)\n",
    "\n",
    "        # We'll use a CNN that downsamples width by factor 4 -> input_lengths = ceil(w/4)\n",
    "        input_lengths = torch.tensor([math.ceil(w / 4) for w in widths], dtype=torch.long)\n",
    "\n",
    "        return Batch(\n",
    "            imgs=padded,\n",
    "            labels=labels_concat,\n",
    "            label_lengths=label_lengths,\n",
    "            input_lengths=input_lengths,\n",
    "            texts=texts,\n",
    "            names=names,\n",
    "            orig_widths=widths\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e414e52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 6) Metrics: CER / WER\n",
    "# --------------------------\n",
    "def levenshtein(a: List[str], b: List[str]) -> int:\n",
    "    # Levenshtein distance for lists of tokens (chars or words)\n",
    "    dp = [[0]*(len(b)+1) for _ in range(len(a)+1)]\n",
    "    for i in range(len(a)+1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len(b)+1):\n",
    "        dp[0][j] = j\n",
    "    for i in range(1, len(a)+1):\n",
    "        for j in range(1, len(b)+1):\n",
    "            cost = 0 if a[i-1]==b[j-1] else 1\n",
    "            dp[i][j] = min(dp[i-1][j]+1, dp[i][j-1]+1, dp[i-1][j-1]+cost)\n",
    "    return dp[-1][-1]\n",
    "\n",
    "def cer(ref: str, hyp: str) -> float:\n",
    "    return levenshtein(list(ref), list(hyp)) / max(1, len(ref))\n",
    "\n",
    "def wer(ref: str, hyp: str) -> float:\n",
    "    return levenshtein(ref.split(), hyp.split()) / max(1, len(ref.split()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
