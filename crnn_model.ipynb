{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3305d4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import string\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "try:\n",
    "    from torchaudio.models.decoder import ctc_decoder\n",
    "    HAS_TORCHAUDIO_BEAM = True\n",
    "except Exception:\n",
    "    HAS_TORCHAUDIO_BEAM = False\n",
    "print(HAS_TORCHAUDIO_BEAM)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f115a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 1) Character set & CTC codec\n",
    "# ----------------------------\n",
    "def default_charset():\n",
    "    # You can customize this to match your data (e.g., only lowercase + space)\n",
    "    # Keep space ' ' included if your lines contain spaces.\n",
    "    charset = list(string.digits + string.ascii_letters + string.punctuation + ' ')\n",
    "    # Remove characters you know you don't have, or add accents if needed.\n",
    "    return charset\n",
    "\n",
    "class CTCCodec:\n",
    "    \"\"\"\n",
    "    Maps characters <-> indices. Index 0 is reserved for CTC blank.\n",
    "    \"\"\"\n",
    "    def __init__(self, charset: List[str]):\n",
    "        self.blank_idx = 0\n",
    "        self.chars = ['<BLK>'] + charset\n",
    "        self.char2idx = {c: i+1 for i, c in enumerate(charset)}  # shift by +1\n",
    "        self.idx2char = {i+1: c for i, c in enumerate(charset)}\n",
    "\n",
    "    def encode(self, text: str) -> torch.Tensor:\n",
    "        return torch.tensor([self.char2idx[c] for c in text if c in self.char2idx], dtype=torch.long)\n",
    "\n",
    "    def decode_greedy(self, logits: torch.Tensor) -> List[str]:\n",
    "        \"\"\"\n",
    "        logits: (T, N, C) log-probs or raw scores. We'll argmax over classes.\n",
    "        Returns list of length N with collapsed CTC decoding.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(dim=-1)  # (T, N)\n",
    "            pred = pred.cpu().numpy()\n",
    "        N = pred.shape[1]\n",
    "        texts = []\n",
    "        for n in range(N):\n",
    "            seq = pred[:, n]\n",
    "            prev = -1\n",
    "            out = []\n",
    "            for idx in seq:\n",
    "                if idx != self.blank_idx and idx != prev:\n",
    "                    out.append(self.idx2char.get(int(idx), ''))\n",
    "                prev = idx\n",
    "            texts.append(''.join(out))\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caf5962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# 2) Image transforms & augmentations\n",
    "# ----------------------------------\n",
    "class KeepRatioResize:\n",
    "    \"\"\"\n",
    "    Resize PIL image to target height with proportional width, no crop.\n",
    "    \"\"\"\n",
    "    def __init__(self, target_h: int):\n",
    "        self.target_h = target_h\n",
    "\n",
    "    def __call__(self, img: Image.Image) -> Image.Image:\n",
    "        w, h = img.size\n",
    "        if h == self.target_h:\n",
    "            return img\n",
    "        new_w = max(1, round(w * (self.target_h / h)))\n",
    "        return img.resize((new_w, self.target_h), Image.BILINEAR)\n",
    "\n",
    "class ElasticLike:\n",
    "    \"\"\"\n",
    "    Lightweight 'elastic' style warp using PIL perspective + slight blur/sharpen.\n",
    "    Keeps text legible but varied.\n",
    "    \"\"\"\n",
    "    def __init__(self, p=0.5, max_warp=0.08):\n",
    "        self.p = p\n",
    "        self.max_warp = max_warp\n",
    "\n",
    "    def __call__(self, img: Image.Image) -> Image.Image:\n",
    "        if random.random() > self.p:\n",
    "            return img\n",
    "        w, h = img.size\n",
    "        dx = int(self.max_warp * w)\n",
    "        dy = int(self.max_warp * h)\n",
    "        # random offsets for corners\n",
    "        src = [(0,0),(w,0),(w,h),(0,h)]\n",
    "        img = img.transform((w + random.randint(-dx, dx), h + random.randint(-dx, dx)), Image.QUAD, src)\n",
    "        if random.random() < 0.5:\n",
    "            img = img.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.2, 0.6)))\n",
    "        if random.random() < 0.3:\n",
    "            img = img.filter(ImageFilter.UnsharpMask(radius=1.0, percent=80, threshold=3))\n",
    "        return img\n",
    "\n",
    "def pil_to_tensor_normalized(img: Image.Image) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert PIL (grayscale) -> Tensor in [0,1], normalize to mean=0.5, std=0.5\n",
    "    Output shape: (1, H, W)\n",
    "    \"\"\"\n",
    "    t = transforms.functional.pil_to_tensor(img).float() / 255.0  # (1,H,W) for 'L'\n",
    "    return transforms.functional.normalize(t, mean=[0.5], std=[0.5])\n",
    "\n",
    "def binarize_if_needed(img: Image.Image, p=0.0):\n",
    "    if p > 0 and random.random() < p:\n",
    "        return img.convert('L').point(lambda x: 255 if x > 200 else 0, mode='L')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37191bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# 3) Dataset definitions\n",
    "# ------------------------\n",
    "class LinesFile(Dataset):\n",
    "    \"\"\"\n",
    "    labels.txt format: path<TAB>text (UTF-8)\n",
    "    Converts to grayscale, resizes to H=64 with proportional width.\n",
    "    \"\"\"\n",
    "    def __init__(self, labels_file: str, codec: CTCCodec, target_h: int = 64, keep_aspect=True, binarize_p=0.0):\n",
    "        super().__init__()\n",
    "        self.samples = []\n",
    "        self.folder_path = os.path.dirname(labels_file)\n",
    "        with open(labels_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.rstrip('\\n')\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                parts = line.split('\\t', 1)\n",
    "                if len(parts) != 2:\n",
    "                    continue\n",
    "                path, text = parts\n",
    "                self.samples.append((path, text))\n",
    "        self.codec = codec\n",
    "        self.target_h = target_h\n",
    "        self.keep_aspect = keep_aspect\n",
    "        self.resize = KeepRatioResize(target_h)\n",
    "        self.binarize_p = binarize_p\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file, text = self.samples[idx]\n",
    "        path = os.path.join(self.folder_path, file)\n",
    "        img = Image.open(path).convert('L')\n",
    "        img = ImageOps.exif_transpose(img)\n",
    "        if self.keep_aspect:\n",
    "            img = self.resize(img)\n",
    "        img = binarize_if_needed(img, self.binarize_p)\n",
    "        tensor = pil_to_tensor_normalized(img)\n",
    "        label = self.codec.encode(text)\n",
    "        return tensor, label, text, os.path.basename(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "602dba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedWrapper(Dataset):\n",
    "    \"\"\"\n",
    "    Wrap a base dataset and apply strong augmentations.\n",
    "    Use multiple instances of this wrapper to grow dataset size to >=5x.\n",
    "    \"\"\"\n",
    "    def __init__(self, base: LinesFile):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        # Compose handwriting-friendly augmentations\n",
    "        self.resize = KeepRatioResize(base.target_h)\n",
    "        self.aug = transforms.RandomChoice([\n",
    "            transforms.RandomAffine(degrees=2, translate=(0.02, 0.03), scale=(0.95, 1.05), shear=(-2, 2), fill=255),\n",
    "            transforms.RandomPerspective(distortion_scale=0.3, p=1.0),\n",
    "        ])\n",
    "        self.colorjitter = transforms.ColorJitter(brightness=0.2, contrast=0.2)\n",
    "        self.elastic = ElasticLike(p=0.7, max_warp=0.06)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tensor, label, text, name = self.base[idx]\n",
    "        # back to PIL to apply augmentations that expect PIL\n",
    "        H = tensor.shape[1]\n",
    "        W = tensor.shape[2]\n",
    "        pil = transforms.functional.to_pil_image(((tensor * 0.5 + 0.5) * 255.0).byte())  # unnormalize for aug\n",
    "        # augment in PIL space\n",
    "        # pil = self.elastic(pil)\n",
    "        pil = self.aug(pil)\n",
    "        pil = self.colorjitter(pil)\n",
    "        # Small random Gaussian blur helps mimic scanning\n",
    "        if random.random() < 0.3:\n",
    "            pil = pil.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.2, 0.7)))\n",
    "        # Ensure size back to desired height (keeps ratio)\n",
    "        pil = self.resize(pil)\n",
    "        # Occasionally invert (handwriting scans vary)\n",
    "        if random.random() < 0.25:\n",
    "            pil = ImageOps.invert(pil)\n",
    "        tensor_aug = pil_to_tensor_normalized(pil)\n",
    "        return tensor_aug, label, text, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24c50a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# 4) Collate: pad widths & build lengths\n",
    "# ---------------------------------------\n",
    "@dataclass\n",
    "class Batch:\n",
    "    imgs: torch.Tensor        # (B, 1, H, Wmax)\n",
    "    labels: torch.Tensor      # (sum_targets,)\n",
    "    label_lengths: torch.Tensor  # (B,)\n",
    "    input_lengths: torch.Tensor  # (B,)  number of time steps per sample after CNN\n",
    "    texts: List[str]\n",
    "    names: List[str]\n",
    "    orig_widths: List[int]\n",
    "\n",
    "class PadCollate:\n",
    "    \"\"\"\n",
    "    Pads each batch to max width (also to multiple of 4) and prepares CTC lengths.\n",
    "    \"\"\"\n",
    "    def __init__(self, multiple_of: int = 4, height: int = 64):\n",
    "        self.multiple_of = multiple_of\n",
    "        self.height = height\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # batch: list of (tensor(1,H,W), label, text, name)\n",
    "        imgs, labels, texts, names, widths = [], [], [], [], []\n",
    "        for t, lab, txt, name in batch:\n",
    "            _, h, w = t.size()\n",
    "            assert h == self.height, f\"Expected height {self.height}, got {h}\"\n",
    "            imgs.append(t)\n",
    "            labels.append(lab)\n",
    "            texts.append(txt)\n",
    "            names.append(name)\n",
    "            widths.append(w)\n",
    "\n",
    "        B = len(imgs)\n",
    "        max_w = max(widths)\n",
    "        # pad to next multiple of self.multiple_of (for CNN width downsampling)\n",
    "        if self.multiple_of > 1:\n",
    "            max_w = int(math.ceil(max_w / self.multiple_of) * self.multiple_of)\n",
    "\n",
    "        padded = torch.full((B, 1, self.height, max_w), fill_value=(0.5 - 0.5)/0.5, dtype=imgs[0].dtype)\n",
    "        # Explanation: because we normalized to mean=0.5, std=0.5,\n",
    "        # \"white\" (1.0) becomes (1-0.5)/0.5 = +1.0, \"gray 0.5\" is 0; but to avoid halo,\n",
    "        # we can pad with normalized value of 1.0 (white) -> +1.0:\n",
    "        padded.fill_(+1.0)\n",
    "\n",
    "        for i, t in enumerate(imgs):\n",
    "            _, _, w = t.size()\n",
    "            padded[i, :, :, :w] = t\n",
    "\n",
    "        labels_concat = torch.cat(labels, dim=0)\n",
    "        label_lengths = torch.tensor([len(l) for l in labels], dtype=torch.long)\n",
    "\n",
    "        # We'll use a CNN that downsamples width by factor 4 -> input_lengths = ceil(w/4)\n",
    "        input_lengths = torch.tensor([math.ceil(w / 4) for w in widths], dtype=torch.long)\n",
    "\n",
    "        return Batch(\n",
    "            imgs=padded,\n",
    "            labels=labels_concat,\n",
    "            label_lengths=label_lengths,\n",
    "            input_lengths=input_lengths,\n",
    "            texts=texts,\n",
    "            names=names,\n",
    "            orig_widths=widths\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2b01884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 5) CRNN model (CNN + BiLSTM)\n",
    "# ---------------------------\n",
    "class CRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN reduces H and W (width by 4x overall), then we pool height to 1 and treat width as time.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes: int, in_channels=1):\n",
    "        super().__init__()\n",
    "        # VGG-ish feature extractor; keep it simple and efficient\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # H/2, W/2\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # H/4, W/4\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            # keep width stride=1 here\n",
    "\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(inplace=True),\n",
    "        )\n",
    "        # Collapse height to 1 with adaptive pooling; width stays ~W/4 due to the two pools above.\n",
    "        self.height_pool = nn.AdaptiveAvgPool2d((1, None))\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=256, hidden_size=256, num_layers=2,\n",
    "            bidirectional=True, dropout=0.1, batch_first=False\n",
    "        )\n",
    "        self.fc = nn.Linear(512, num_classes)  # 2*hidden\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B,1,H,W)\n",
    "        f = self.features(x)                        # (B, C=256, H', W')\n",
    "        f = self.height_pool(f).squeeze(2)          # (B, C, W')\n",
    "        f = f.permute(2, 0, 1)                      # (T=W', B, C)\n",
    "        out, _ = self.rnn(f)                        # (T, B, 2*H)\n",
    "        logits = self.fc(out)                       # (T, B, num_classes)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e414e52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 6) Metrics: CER / WER\n",
    "# --------------------------\n",
    "def levenshtein(a: List[str], b: List[str]) -> int:\n",
    "    # Levenshtein distance for lists of tokens (chars or words)\n",
    "    dp = [[0]*(len(b)+1) for _ in range(len(a)+1)]\n",
    "    for i in range(len(a)+1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len(b)+1):\n",
    "        dp[0][j] = j\n",
    "    for i in range(1, len(a)+1):\n",
    "        for j in range(1, len(b)+1):\n",
    "            cost = 0 if a[i-1]==b[j-1] else 1\n",
    "            dp[i][j] = min(dp[i-1][j]+1, dp[i][j-1]+1, dp[i-1][j-1]+cost)\n",
    "    return dp[-1][-1]\n",
    "\n",
    "def cer(ref: str, hyp: str) -> float:\n",
    "    return levenshtein(list(ref), list(hyp)) / max(1, len(ref))\n",
    "\n",
    "def wer(ref: str, hyp: str) -> float:\n",
    "    return levenshtein(ref.split(), hyp.split()) / max(1, len(ref.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb74535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_beam_decoder(codec):\n",
    "    \"\"\"\n",
    "    Build a token-level CTC beam search decoder (no lexicon / no LM).\n",
    "    Returns None if torchaudio beam is unavailable.\n",
    "    \"\"\"\n",
    "    if not HAS_TORCHAUDIO_BEAM:\n",
    "        return None\n",
    "    # tokens must be index-aligned with model outputs where 0 is blank\n",
    "    tokens = [\"<blk>\"] + [codec.idx2char[i] for i in sorted(codec.idx2char)]\n",
    "    return ctc_decoder(\n",
    "        lexicon=None,        # token-level beam (no lexicon)\n",
    "        tokens=tokens,\n",
    "        beam_size=10,        # try 5–20; 10 is a good start\n",
    "    )\n",
    "\n",
    "def decode_beam_batch(log_probs, codec, beam_decoder):\n",
    "    \"\"\"\n",
    "    log_probs: (T, B, C) on CPU\n",
    "    Returns: List[str] length B\n",
    "    \"\"\"\n",
    "    results = beam_decoder(log_probs)  # list of length B; each is [best, ...]\n",
    "    hyps = []\n",
    "    for beams in results:\n",
    "        best = beams[0]\n",
    "        # best.tokens are class indices; 0 is blank. They are already CTC-collapsed.\n",
    "        try:\n",
    "            seq = [codec.idx2char[i] for i in best.tokens if i != codec.blank_idx]\n",
    "        except KeyError:\n",
    "            seq = []\n",
    "        hyps.append(''.join(seq))\n",
    "    return hyps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e19444b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 7) Training / Validation\n",
    "# --------------------------\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    labels_file: str = os.path.join(\"images\", \"labels.txt\")\n",
    "    batch_size: int = 16\n",
    "    epochs: int = 30\n",
    "    lr: float = 1e-3\n",
    "    num_workers: int = 4\n",
    "    height: int = 64\n",
    "    seed: int = 42\n",
    "    aug_factor: int = 4   # original + 4× augmented = 5× total\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def make_dataloaders(cfg: TrainConfig, codec: CTCCodec):\n",
    "    # Load all samples once\n",
    "    all_data = []\n",
    "    with open(cfg.labels_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip('\\n')\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            parts = line.split('\\t', 1)\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            all_data.append(line)\n",
    "\n",
    "    train_lines, val_lines = train_test_split(all_data, test_size=0.1, random_state=cfg.seed, shuffle=True)\n",
    "\n",
    "    # Save temporary split files\n",
    "    train_file = os.path.join(\"images\", \"train_split.txt\")\n",
    "    val_file = os.path.join(\"images\", \"val_split.txt\")\n",
    "    with open(train_file, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(train_lines))\n",
    "    with open(val_file, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(val_lines))\n",
    "\n",
    "    # Create datasets\n",
    "    base_train = LinesFile(train_file, codec, target_h=cfg.height, keep_aspect=True)\n",
    "    aug_wrappers = [AugmentedWrapper(base_train) for _ in range(cfg.aug_factor)]\n",
    "    train_set = ConcatDataset([base_train] + aug_wrappers)\n",
    "\n",
    "    val_set = LinesFile(val_file, codec, target_h=cfg.height, keep_aspect=True)\n",
    "\n",
    "    collate = PadCollate(multiple_of=4, height=cfg.height)\n",
    "    train_loader = DataLoader(train_set, batch_size=cfg.batch_size, shuffle=True,\n",
    "                              num_workers=cfg.num_workers, pin_memory=True, collate_fn=collate)\n",
    "    val_loader = DataLoader(val_set, batch_size=cfg.batch_size, shuffle=False,\n",
    "                            num_workers=cfg.num_workers, pin_memory=True, collate_fn=collate)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device, codec: CTCCodec, log_interval=100):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for step, batch in enumerate(loader, 1):\n",
    "        imgs = batch.imgs.to(device)\n",
    "        labels = batch.labels.to(device)\n",
    "        label_lengths = batch.label_lengths.to(device)\n",
    "        input_lengths = batch.input_lengths.to(device)\n",
    "\n",
    "        logits = model(imgs)  # (T, B, C)\n",
    "        log_probs = logits.log_softmax(dim=-1)\n",
    "\n",
    "        loss = criterion(log_probs, labels, input_lengths, label_lengths)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if step % log_interval == 0:\n",
    "            avg = running_loss / log_interval\n",
    "            print(f\"  step {step:5d} | train loss {avg:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "def validate(model, loader, device, codec: CTCCodec):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Greedy metrics\n",
    "    total_cer_g = 0.0\n",
    "    total_wer_g = 0.0\n",
    "\n",
    "    # Beam metrics\n",
    "    beam_decoder = build_beam_decoder(codec)\n",
    "    total_cer_b = 0.0\n",
    "    total_wer_b = 0.0\n",
    "    count = 0\n",
    "\n",
    "    criterion = nn.CTCLoss(blank=codec.blank_idx, zero_infinity=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            if batch is None:\n",
    "                continue\n",
    "            imgs = batch.imgs.to(device)\n",
    "            labels = batch.labels.to(device)\n",
    "            label_lengths = batch.label_lengths.to(device)\n",
    "            input_lengths = batch.input_lengths.to(device)\n",
    "\n",
    "            logits = model(imgs)\n",
    "            log_probs = logits.log_softmax(dim=-1)\n",
    "            loss = criterion(log_probs, labels, input_lengths, label_lengths)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Greedy decode for metrics\n",
    "            hyps_g = codec.decode_greedy(log_probs)\n",
    "            for hyp, ref in zip(hyps_g, batch.texts):\n",
    "                total_cer_g += cer(ref, hyp)\n",
    "                total_wer_g += wer(ref, hyp)\n",
    "                \n",
    "            # Beam decode\n",
    "            if beam_decoder is not None:\n",
    "                hyps_b = decode_beam_batch(log_probs.cpu(), codec, beam_decoder)\n",
    "                for hyp, ref in zip(hyps_b, batch.texts):\n",
    "                    total_cer_b += cer(ref, hyp)\n",
    "                    total_wer_b += wer(ref, hyp)\n",
    "\n",
    "            count += len(batch.texts)\n",
    "\n",
    "    # Averages\n",
    "    metrics = {\n",
    "        \"loss\": total_loss / max(1, len(loader)),\n",
    "        \"greedy\": {\n",
    "            \"cer\": total_cer_g / max(1, count),\n",
    "            \"wer\": total_wer_g / max(1, count),\n",
    "        },\n",
    "        \"beam\": None\n",
    "    }\n",
    "    if beam_decoder is not None:\n",
    "        metrics[\"beam\"] = {\n",
    "            \"cer\": total_cer_b / max(1, count),\n",
    "            \"wer\": total_wer_b / max(1, count),\n",
    "        }\n",
    "\n",
    "    # Pretty print for quick comparison\n",
    "    if metrics[\"beam\"] is not None:\n",
    "        print(f\"  Val loss: {metrics['loss']:.4f} | \"\n",
    "              f\"Greedy CER {metrics['greedy']['cer']:.4f} WER {metrics['greedy']['wer']:.4f} | \"\n",
    "              f\"Beam CER {metrics['beam']['cer']:.4f} WER {metrics['beam']['wer']:.4f}\")\n",
    "    else:\n",
    "        print(f\"  Val loss: {metrics['loss']:.4f} | \"\n",
    "              f\"Greedy CER {metrics['greedy']['cer']:.4f} WER {metrics['greedy']['wer']:.4f} \"\n",
    "              f\"(torchaudio beam not available)\")\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f6d6751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop():\n",
    "    cfg = TrainConfig(\n",
    "        labels_file=os.path.join(\"images\", \"labels.txt\"),\n",
    "        batch_size=16,\n",
    "        epochs=30,\n",
    "        lr=1e-3,\n",
    "        num_workers=0,\n",
    "        height=64,\n",
    "        seed=42,\n",
    "        aug_factor=5,\n",
    "    )\n",
    "\n",
    "    set_seed(cfg.seed)\n",
    "\n",
    "    charset = default_charset()\n",
    "    codec = CTCCodec(charset)\n",
    "    train_loader, val_loader = make_dataloaders(cfg, codec)\n",
    "\n",
    "    num_classes = 1 + len(charset)\n",
    "    model = CRNN(num_classes=num_classes, in_channels=1).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=cfg.lr, steps_per_epoch=len(train_loader), epochs=cfg.epochs\n",
    "    )\n",
    "    criterion = nn.CTCLoss(blank=codec.blank_idx, zero_infinity=True)\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{cfg.epochs}\")\n",
    "        train_one_epoch(model, train_loader, criterion, optimizer, device, codec, log_interval=100)\n",
    "        metrics = validate(model, val_loader, device, codec)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"  Val loss: {metrics['loss']:.4f} | CER: {metrics['cer']:.4f} | WER: {metrics['wer']:.4f}\")\n",
    "\n",
    "        if metrics['loss'] < best_val:\n",
    "            best_val = metrics['loss']\n",
    "            torch.save({\n",
    "                \"model\": model.state_dict(),\n",
    "                \"codec_chars\": codec.chars,\n",
    "                \"config\": cfg.__dict__,\n",
    "            }, \"best_crnn_ctc.pth\")\n",
    "            print(\"  Saved checkpoint: best_crnn_ctc.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3327b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESUME_PATH = \"best_crnn_ctc_47_epochs_0_0441_CER_0_2147_WER.pth\"\n",
    "TARGET_EPOCHS = 100\n",
    "\n",
    "def resume_training():\n",
    "    set_seed(42)\n",
    "\n",
    "    # 1) Rebuild codec/model from checkpoint\n",
    "    ckpt = torch.load(RESUME_PATH, map_location=\"cpu\")\n",
    "    codec = CTCCodec(ckpt[\"codec_chars\"][1:])  # strip <BLK> placeholder\n",
    "\n",
    "    # 2) Re-create config & loaders (same labels.txt split logic as before)\n",
    "    cfg = TrainConfig(\n",
    "        labels_file=os.path.join(\"images\", \"labels.txt\"),\n",
    "        batch_size=16,\n",
    "        epochs=TARGET_EPOCHS,  # we'll run until 100\n",
    "        lr=5e-4,               # smaller LR for continued training\n",
    "        num_workers=0,\n",
    "        height=64,\n",
    "        seed=42,\n",
    "        aug_factor=5,\n",
    "    )\n",
    "    train_loader, val_loader = make_dataloaders(cfg, codec)\n",
    "\n",
    "    # 3) Rebuild model and load weights\n",
    "    num_classes = len(ckpt[\"codec_chars\"])\n",
    "    model = CRNN(num_classes=num_classes, in_channels=1).to(device)\n",
    "    model.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "\n",
    "    # 4) Fresh optimizer/scheduler for the new phase\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=1e-4)\n",
    "    # Simple scheduler that’s easy to resume mid-run:\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader)* (TARGET_EPOCHS//5 + 1))\n",
    "    criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_cer = float(\"inf\")\n",
    "    best_wer = float(\"inf\")\n",
    "    best_score = float(\"inf\")\n",
    "    # If you remember where you stopped (say epoch=30), you can set start_epoch=31.\n",
    "    # Otherwise just keep going; training is robust to extra epochs with low LR.\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        print(f\"\\n[Resume Phase] Epoch {epoch}/{cfg.epochs}\")\n",
    "        train_one_epoch(model, train_loader, criterion, optimizer, device, codec, log_interval=100)\n",
    "        metrics = validate(model, val_loader, device, codec)\n",
    "        scheduler.step()\n",
    "\n",
    "        cer_metric = metrics['beam']['cer'] if HAS_TORCHAUDIO_BEAM else metrics['greedy']['cer']\n",
    "        wer_metric = metrics['beam']['wer'] if HAS_TORCHAUDIO_BEAM else metrics['greedy']['wer']\n",
    "        # cer_metric = metrics['greedy']['cer']\n",
    "        # wer_metric = metrics['greedy']['wer']\n",
    "\n",
    "        if metrics['loss'] < best_val:\n",
    "            best_val = metrics['loss']\n",
    "            torch.save({\n",
    "                \"model\": model.state_dict(),\n",
    "                \"codec_chars\": codec.chars,\n",
    "                \"config\": cfg.__dict__,\n",
    "            }, \"best_crnn_ctc_val.pth\")\n",
    "            print(\"  Saved: best_crnn_ctc_val.pth\")\n",
    "\n",
    "        if cer_metric < best_cer:\n",
    "            best_cer = cer_metric\n",
    "            torch.save({\n",
    "                \"model\": model.state_dict(),\n",
    "                \"codec_chars\": codec.chars,\n",
    "                \"config\": cfg.__dict__,\n",
    "            }, \"best_crnn_ctc_cer.pth\")\n",
    "            print(\"  Saved: best_crnn_ctc_cer.pth\")\n",
    "\n",
    "        if wer_metric < best_wer:\n",
    "            best_wer = wer_metric\n",
    "            torch.save({\n",
    "                \"model\": model.state_dict(),\n",
    "                \"codec_chars\": codec.chars,\n",
    "                \"config\": cfg.__dict__,\n",
    "            }, \"best_crnn_ctc_wer.pth\")\n",
    "            print(\"  Saved: best_crnn_ctc_wer.pth\")\n",
    "\n",
    "        score = cer_metric + 0.5 * wer_metric\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            torch.save({\n",
    "                \"model\": model.state_dict(),\n",
    "                \"codec_chars\": codec.chars,\n",
    "                \"config\": cfg.__dict__,\n",
    "            }, \"best_crnn_ctc_score.pth\")\n",
    "            print(\"  Saved: best_crnn_ctc_score.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3404de85",
   "metadata": {},
   "source": [
    "**Inference code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2be2843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Load model checkpoint ----------\n",
    "def load_model(checkpoint_path, device):\n",
    "    ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "    codec = CTCCodec(ckpt[\"codec_chars\"][1:])  # skip <BLK> token\n",
    "    model = CRNN(num_classes=len(ckpt[\"codec_chars\"]), in_channels=1).to(device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    model.eval()\n",
    "    return model, codec\n",
    "\n",
    "# ---------- Preprocess single image ----------\n",
    "def preprocess_image(img_path, target_h=64):\n",
    "    img = Image.open(img_path).convert(\"L\")\n",
    "    img = ImageOps.exif_transpose(img)\n",
    "    resize = KeepRatioResize(target_h)\n",
    "    img = resize(img)\n",
    "    tensor = pil_to_tensor_normalized(img).unsqueeze(0)  # (1,1,H,W)\n",
    "    return tensor, img.size  # (W,H)\n",
    "\n",
    "# ---------- Decode prediction ----------\n",
    "def predict(model, codec, img_tensor, device):\n",
    "    with torch.no_grad():\n",
    "        img_tensor = img_tensor.to(device)\n",
    "        logits = model(img_tensor)         # (T, B, C)\n",
    "        log_probs = logits.log_softmax(dim=-1)\n",
    "        if HAS_TORCHAUDIO_BEAM:\n",
    "            beam_decoder = build_beam_decoder(codec)\n",
    "            text = decode_beam_batch(log_probs.cpu(), codec, beam_decoder)[0]\n",
    "        else:\n",
    "            text = codec.decode_greedy(log_probs)[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50d81d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Resume Phase] Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antpa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step   100 | train loss 0.0666\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "tensor(92)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# train_loop()\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43mresume_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# # Inference\u001b[39;00m\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# model_path = \"best_crnn_ctc_53_epochs_0_0391_CER_0_1841_WER.pth\"\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# # test_image = os.path.join(\"sample_images\", \"20251103_061143.jpg\")\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m \n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# print(f\"\\nPredicted text:\\n{pred_text}\")\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mresume_training\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[Resume Phase] Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg.epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m train_one_epoch(model, train_loader, criterion, optimizer, device, codec, log_interval=\u001b[32m100\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m metrics = \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m scheduler.step()\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# cer_metric = metrics['beam']['cer'] if HAS_TORCHAUDIO_BEAM else metrics['greedy']['cer']\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# wer_metric = metrics['beam']['wer'] if HAS_TORCHAUDIO_BEAM else metrics['greedy']['wer']\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 119\u001b[39m, in \u001b[36mvalidate\u001b[39m\u001b[34m(model, loader, device, codec)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# Beam decode\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m beam_decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     hyps_b = \u001b[43mdecode_beam_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_decoder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hyp, ref \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(hyps_b, batch.texts):\n\u001b[32m    121\u001b[39m         total_cer_b += cer(ref, hyp)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mdecode_beam_batch\u001b[39m\u001b[34m(log_probs, codec, beam_decoder)\u001b[39m\n\u001b[32m     24\u001b[39m     best = beams[\u001b[32m0\u001b[39m]\n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# best.tokens are class indices; 0 is blank. They are already CTC-collapsed.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     seq = \u001b[43m[\u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m.\u001b[49m\u001b[43midx2char\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mblank_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     27\u001b[39m     hyps.append(\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m.join(seq))\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hyps\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     24\u001b[39m     best = beams[\u001b[32m0\u001b[39m]\n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# best.tokens are class indices; 0 is blank. They are already CTC-collapsed.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     seq = [\u001b[43mcodec\u001b[49m\u001b[43m.\u001b[49m\u001b[43midx2char\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m best.tokens \u001b[38;5;28;01mif\u001b[39;00m i != codec.blank_idx]\n\u001b[32m     27\u001b[39m     hyps.append(\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m.join(seq))\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hyps\n",
      "\u001b[31mKeyError\u001b[39m: tensor(92)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # train_loop()\n",
    "\n",
    "    resume_training()\n",
    "\n",
    "    # # Inference\n",
    "    # model_path = \"best_crnn_ctc_53_epochs_0_0391_CER_0_1841_WER.pth\"\n",
    "    # # test_image = os.path.join(\"sample_images\", \"20251103_061143.jpg\")\n",
    "    # test_image = os.path.join(\"sample_images\", \"20251103_061143.jpg\")\n",
    "\n",
    "    # print(f\"Loading model from {model_path} ...\")\n",
    "    # model, codec = load_model(model_path, device)\n",
    "\n",
    "    # print(f\"Running inference on {test_image} ...\")\n",
    "    # img_tensor, (w, h) = preprocess_image(test_image, target_h=64)\n",
    "    # pred_text = predict(model, codec, img_tensor, device)\n",
    "\n",
    "    # print(f\"\\nPredicted text:\\n{pred_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
