{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3305d4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import string\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from char_trigram_lm import ctc_beam_search_with_char_lm\n",
    "from char_trigram_lm import CharTrigramLM\n",
    "from char_trigram_lm import load_train_texts_from_labels\n",
    "\n",
    "from lines_splitter import get_lines\n",
    "\n",
    "try:\n",
    "    from torchaudio.models.decoder import ctc_decoder\n",
    "    HAS_TORCHAUDIO_BEAM = True\n",
    "except Exception:\n",
    "    HAS_TORCHAUDIO_BEAM = False\n",
    "print(HAS_TORCHAUDIO_BEAM)\n",
    "HAS_TORCHAUDIO_BEAM = False  # Temporarily disable torchaudio beam search\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f115a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 1) Character set & CTC codec\n",
    "# ----------------------------\n",
    "def default_charset():\n",
    "    # You can customize this to match your data (e.g., only lowercase + space)\n",
    "    # Keep space ' ' included if your lines contain spaces.\n",
    "    charset = list(string.digits + string.ascii_letters + string.punctuation + ' ')\n",
    "    # Remove characters you know you don't have, or add accents if needed.\n",
    "    return charset\n",
    "\n",
    "class CTCCodec:\n",
    "    \"\"\"\n",
    "    Maps characters <-> indices. Index 0 is reserved for CTC blank.\n",
    "    \"\"\"\n",
    "    def __init__(self, charset: List[str]):\n",
    "        self.blank_idx = 0\n",
    "        self.chars = ['<BLK>'] + charset\n",
    "        self.char2idx = {c: i+1 for i, c in enumerate(charset)}  # shift by +1\n",
    "        self.idx2char = {i+1: c for i, c in enumerate(charset)}\n",
    "\n",
    "    def encode(self, text: str) -> torch.Tensor:\n",
    "        return torch.tensor([self.char2idx[c] for c in text if c in self.char2idx], dtype=torch.long)\n",
    "\n",
    "    def decode_greedy(self, logits: torch.Tensor) -> List[str]:\n",
    "        \"\"\"\n",
    "        logits: (T, N, C) log-probs or raw scores. We'll argmax over classes.\n",
    "        Returns list of length N with collapsed CTC decoding.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(dim=-1)  # (T, N)\n",
    "            pred = pred.cpu().numpy()\n",
    "        N = pred.shape[1]\n",
    "        texts = []\n",
    "        for n in range(N):\n",
    "            seq = pred[:, n]\n",
    "            prev = -1\n",
    "            out = []\n",
    "            for idx in seq:\n",
    "                if idx != self.blank_idx and idx != prev:\n",
    "                    out.append(self.idx2char.get(int(idx), ''))\n",
    "                prev = idx\n",
    "            texts.append(''.join(out))\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caf5962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# 2) Image transforms & augmentations\n",
    "# ----------------------------------\n",
    "class KeepRatioResize:\n",
    "    \"\"\"\n",
    "    Resize PIL image to target height with proportional width, no crop.\n",
    "    \"\"\"\n",
    "    def __init__(self, target_h: int):\n",
    "        self.target_h = target_h\n",
    "\n",
    "    def __call__(self, img: Image.Image) -> Image.Image:\n",
    "        w, h = img.size\n",
    "        if h == self.target_h:\n",
    "            return img\n",
    "        new_w = max(1, round(w * (self.target_h / h)))\n",
    "        return img.resize((new_w, self.target_h), Image.BILINEAR)\n",
    "\n",
    "class ElasticLike:\n",
    "    \"\"\"\n",
    "    Lightweight 'elastic' style warp using PIL perspective + slight blur/sharpen.\n",
    "    Keeps text legible but varied.\n",
    "    \"\"\"\n",
    "    def __init__(self, p=0.5, max_warp=0.08):\n",
    "        self.p = p\n",
    "        self.max_warp = max_warp\n",
    "\n",
    "    def __call__(self, img: Image.Image) -> Image.Image:\n",
    "        if random.random() > self.p:\n",
    "            return img\n",
    "        w, h = img.size\n",
    "        dx = int(self.max_warp * w)\n",
    "        dy = int(self.max_warp * h)\n",
    "        # random offsets for corners\n",
    "        src = [(0,0),(w,0),(w,h),(0,h)]\n",
    "        img = img.transform((w + random.randint(-dx, dx), h + random.randint(-dx, dx)), Image.QUAD, src)\n",
    "        if random.random() < 0.5:\n",
    "            img = img.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.2, 0.6)))\n",
    "        if random.random() < 0.3:\n",
    "            img = img.filter(ImageFilter.UnsharpMask(radius=1.0, percent=80, threshold=3))\n",
    "        return img\n",
    "\n",
    "def pil_to_tensor_normalized(img: Image.Image) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert PIL (grayscale) -> Tensor in [0,1], normalize to mean=0.5, std=0.5\n",
    "    Output shape: (1, H, W)\n",
    "    \"\"\"\n",
    "    t = transforms.functional.pil_to_tensor(img).float() / 255.0  # (1,H,W) for 'L'\n",
    "    return transforms.functional.normalize(t, mean=[0.5], std=[0.5])\n",
    "\n",
    "def binarize_if_needed(img: Image.Image, p=0.0):\n",
    "    if p > 0 and random.random() < p:\n",
    "        return img.convert('L').point(lambda x: 255 if x > 200 else 0, mode='L')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37191bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# 3) Dataset definitions\n",
    "# ------------------------\n",
    "class LinesFile(Dataset):\n",
    "    \"\"\"\n",
    "    labels.txt format: path<TAB>text (UTF-8)\n",
    "    Converts to grayscale, resizes to H=64 with proportional width.\n",
    "    \"\"\"\n",
    "    def __init__(self, labels_file: str, codec: CTCCodec, target_h: int = 64, keep_aspect=True, binarize_p=0.0):\n",
    "        super().__init__()\n",
    "        self.samples = []\n",
    "        self.folder_path = os.path.dirname(labels_file)\n",
    "        with open(labels_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.rstrip('\\n')\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                parts = line.split('\\t', 1)\n",
    "                if len(parts) != 2:\n",
    "                    continue\n",
    "                path, text = parts\n",
    "                self.samples.append((path, text))\n",
    "        self.codec = codec\n",
    "        self.target_h = target_h\n",
    "        self.keep_aspect = keep_aspect\n",
    "        self.resize = KeepRatioResize(target_h)\n",
    "        self.binarize_p = binarize_p\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file, text = self.samples[idx]\n",
    "        path = os.path.join(self.folder_path, file)\n",
    "        img = Image.open(path).convert('L')\n",
    "        img = ImageOps.exif_transpose(img)\n",
    "        if self.keep_aspect:\n",
    "            img = self.resize(img)\n",
    "        img = binarize_if_needed(img, self.binarize_p)\n",
    "        tensor = pil_to_tensor_normalized(img)\n",
    "        label = self.codec.encode(text)\n",
    "        return tensor, label, text, os.path.basename(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "602dba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedWrapper(Dataset):\n",
    "    \"\"\"\n",
    "    Wrap a base dataset and apply strong augmentations.\n",
    "    Use multiple instances of this wrapper to grow dataset size to >=5x.\n",
    "    \"\"\"\n",
    "    def __init__(self, base: LinesFile):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        # Compose handwriting-friendly augmentations\n",
    "        self.resize = KeepRatioResize(base.target_h)\n",
    "        self.aug = transforms.RandomChoice([\n",
    "            transforms.RandomAffine(degrees=2, translate=(0.02, 0.03), scale=(0.95, 1.05), shear=(-2, 2), fill=255),\n",
    "            transforms.RandomPerspective(distortion_scale=0.3, p=1.0),\n",
    "        ])\n",
    "        self.colorjitter = transforms.ColorJitter(brightness=0.2, contrast=0.2)\n",
    "        self.elastic = ElasticLike(p=0.7, max_warp=0.06)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tensor, label, text, name = self.base[idx]\n",
    "        # back to PIL to apply augmentations that expect PIL\n",
    "        H = tensor.shape[1]\n",
    "        W = tensor.shape[2]\n",
    "        pil = transforms.functional.to_pil_image(((tensor * 0.5 + 0.5) * 255.0).byte())  # unnormalize for aug\n",
    "        # augment in PIL space\n",
    "        # pil = self.elastic(pil)  # kinda exception in inner library\n",
    "        pil = self.aug(pil)\n",
    "        pil = self.colorjitter(pil)\n",
    "        # Small random Gaussian blur helps mimic scanning\n",
    "        if random.random() < 0.3:\n",
    "            pil = pil.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.2, 0.7)))\n",
    "        # Ensure size back to desired height (keeps ratio)\n",
    "        pil = self.resize(pil)\n",
    "        # Occasionally invert (handwriting scans vary)\n",
    "        if random.random() < 0.25:\n",
    "            pil = ImageOps.invert(pil)\n",
    "        tensor_aug = pil_to_tensor_normalized(pil)\n",
    "        return tensor_aug, label, text, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24c50a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# 4) Collate: pad widths & build lengths\n",
    "# ---------------------------------------\n",
    "@dataclass\n",
    "class Batch:\n",
    "    imgs: torch.Tensor        # (B, 1, H, Wmax)\n",
    "    labels: torch.Tensor      # (sum_targets,)\n",
    "    label_lengths: torch.Tensor  # (B,)\n",
    "    input_lengths: torch.Tensor  # (B,)  number of time steps per sample after CNN\n",
    "    texts: List[str]\n",
    "    names: List[str]\n",
    "    orig_widths: List[int]\n",
    "\n",
    "class PadCollate:\n",
    "    \"\"\"\n",
    "    Pads each batch to max width (also to multiple of 4) and prepares CTC lengths.\n",
    "    \"\"\"\n",
    "    def __init__(self, multiple_of: int = 4, height: int = 64):\n",
    "        self.multiple_of = multiple_of\n",
    "        self.height = height\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # batch: list of (tensor(1,H,W), label, text, name)\n",
    "        imgs, labels, texts, names, widths = [], [], [], [], []\n",
    "        for t, lab, txt, name in batch:\n",
    "            _, h, w = t.size()\n",
    "            assert h == self.height, f\"Expected height {self.height}, got {h}\"\n",
    "            imgs.append(t)\n",
    "            labels.append(lab)\n",
    "            texts.append(txt)\n",
    "            names.append(name)\n",
    "            widths.append(w)\n",
    "\n",
    "        B = len(imgs)\n",
    "        max_w = max(widths)\n",
    "        # pad to next multiple of self.multiple_of (for CNN width downsampling)\n",
    "        if self.multiple_of > 1:\n",
    "            max_w = int(math.ceil(max_w / self.multiple_of) * self.multiple_of)\n",
    "\n",
    "        padded = torch.full((B, 1, self.height, max_w), fill_value=(0.5 - 0.5)/0.5, dtype=imgs[0].dtype)\n",
    "        # Explanation: because we normalized to mean=0.5, std=0.5,\n",
    "        # \"white\" (1.0) becomes (1-0.5)/0.5 = +1.0, \"gray 0.5\" is 0; but to avoid halo,\n",
    "        # we can pad with normalized value of 1.0 (white) -> +1.0:\n",
    "        padded.fill_(+1.0)\n",
    "\n",
    "        for i, t in enumerate(imgs):\n",
    "            _, _, w = t.size()\n",
    "            padded[i, :, :, :w] = t\n",
    "\n",
    "        labels_concat = torch.cat(labels, dim=0)\n",
    "        label_lengths = torch.tensor([len(l) for l in labels], dtype=torch.long)\n",
    "\n",
    "        # We'll use a CNN that downsamples width by factor 4 -> input_lengths = ceil(w/4)\n",
    "        input_lengths = torch.tensor([math.ceil(w / 4) for w in widths], dtype=torch.long)\n",
    "\n",
    "        return Batch(\n",
    "            imgs=padded,\n",
    "            labels=labels_concat,\n",
    "            label_lengths=label_lengths,\n",
    "            input_lengths=input_lengths,\n",
    "            texts=texts,\n",
    "            names=names,\n",
    "            orig_widths=widths\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2b01884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 5) CRNN model (CNN + BiLSTM)\n",
    "# ---------------------------\n",
    "# class CRNN(nn.Module):\n",
    "#     \"\"\"\n",
    "#     CNN reduces H and W (width by 4x overall), then we pool height to 1 and treat width as time.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, num_classes: int, in_channels=1):\n",
    "#         super().__init__()\n",
    "#         # VGG-ish feature extractor; keep it simple and efficient\n",
    "#         self.features = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, 64, 3, padding=1), nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),  # H/2, W/2\n",
    "\n",
    "#             nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),  # H/4, W/4\n",
    "\n",
    "#             nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(inplace=True),\n",
    "#             # keep width stride=1 here\n",
    "\n",
    "#             nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(inplace=True),\n",
    "#         )\n",
    "#         # Collapse height to 1 with adaptive pooling; width stays ~W/4 due to the two pools above.\n",
    "#         self.height_pool = nn.AdaptiveAvgPool2d((1, None))\n",
    "\n",
    "#         self.rnn = nn.LSTM(\n",
    "#             input_size=256, hidden_size=256, num_layers=2,\n",
    "#             bidirectional=True, dropout=0.1, batch_first=False\n",
    "#         )\n",
    "#         self.fc = nn.Linear(512, num_classes)  # 2*hidden\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         # x: (B,1,H,W)\n",
    "#         f = self.features(x)                        # (B, C=256, H', W')\n",
    "#         f = self.height_pool(f).squeeze(2)          # (B, C, W')\n",
    "#         f = f.permute(2, 0, 1)                      # (T=W', B, C)\n",
    "#         out, _ = self.rnn(f)                        # (T, B, 2*H)\n",
    "#         logits = self.fc(out)                       # (T, B, num_classes)\n",
    "#         return logits\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, c):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(c, c, 3, padding=1, bias=False)\n",
    "        self.gn1   = nn.GroupNorm(num_groups=min(32, c), num_channels=c)\n",
    "        self.conv2 = nn.Conv2d(c, c, 3, padding=1, bias=False)\n",
    "        self.gn2   = nn.GroupNorm(num_groups=min(32, c), num_channels=c)\n",
    "        self.act   = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.conv1(x); x = self.gn1(x); x = self.act(x)\n",
    "        x = self.conv2(x); x = self.gn2(x)\n",
    "        x = x + residual\n",
    "        return self.act(x)\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    ResCNN (downsample H/4, W/4) -> AdaptivePool(H=1) -> TemporalConv1d -> BiLSTM -> Linear\n",
    "    Keeps width stride ~4 overall so CTC lengths = ceil(W/4).\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes: int, in_channels=1, lstm_hidden=384, lstm_layers=2, lstm_dropout=0.1):\n",
    "        super().__init__()\n",
    "        def block(in_c, out_c, downsample=False):\n",
    "            layers = []\n",
    "            stride = (2,2) if downsample else (1,1)\n",
    "            layers += [nn.Conv2d(in_c, out_c, 3, stride=stride, padding=1, bias=False),\n",
    "                       nn.GroupNorm(num_groups=min(32, out_c), num_channels=out_c),\n",
    "                       nn.ReLU(inplace=True)]\n",
    "            layers += [ResBlock(out_c)]\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        # Downsample twice total (H/2 W/2) -> (H/4 W/4)\n",
    "        self.stage1 = block(in_channels, 64, downsample=True)   # H/2, W/2\n",
    "        self.stage2 = block(64, 128, downsample=True)           # H/4, W/4\n",
    "        self.stage3 = block(128, 256, downsample=False)         # H/4, W/4\n",
    "        self.stage4 = block(256, 512, downsample=False)         # H/4, W/4\n",
    "\n",
    "        self.height_pool = nn.AdaptiveAvgPool2d((1, None))      # (B,512,1,W')\n",
    "        self.temporal_conv = nn.Conv1d(512, 512, kernel_size=5, padding=2)  # (B,512,W')\n",
    "        self.temporal_act  = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=512,\n",
    "            hidden_size=lstm_hidden,\n",
    "            num_layers=lstm_layers,\n",
    "            bidirectional=True,\n",
    "            dropout=lstm_dropout,\n",
    "            batch_first=False\n",
    "        )\n",
    "        self.fc = nn.Linear(2*lstm_hidden, num_classes)\n",
    "\n",
    "    def forward(self, x):             # x: (B,1,64,W)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)            # (B,512,H',W') with H' = 64/4=16\n",
    "        x = self.height_pool(x).squeeze(2)   # (B,512,W')\n",
    "        x = self.temporal_conv(x)            # (B,512,W')\n",
    "        x = self.temporal_act(x)\n",
    "        x = x.permute(2,0,1)                 # (T=W', B, 512)\n",
    "        x, _ = self.rnn(x)                   # (T,B,2*hidden)\n",
    "        x = self.fc(x)                       # (T,B,C)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e414e52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 6) Metrics: CER / WER\n",
    "# --------------------------\n",
    "def levenshtein(a: List[str], b: List[str]) -> int:\n",
    "    # Levenshtein distance for lists of tokens (chars or words)\n",
    "    dp = [[0]*(len(b)+1) for _ in range(len(a)+1)]\n",
    "    for i in range(len(a)+1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len(b)+1):\n",
    "        dp[0][j] = j\n",
    "    for i in range(1, len(a)+1):\n",
    "        for j in range(1, len(b)+1):\n",
    "            cost = 0 if a[i-1]==b[j-1] else 1\n",
    "            dp[i][j] = min(dp[i-1][j]+1, dp[i][j-1]+1, dp[i-1][j-1]+cost)\n",
    "    return dp[-1][-1]\n",
    "\n",
    "def cer(ref: str, hyp: str) -> float:\n",
    "    return levenshtein(list(ref), list(hyp)) / max(1, len(ref))\n",
    "\n",
    "def wer(ref: str, hyp: str) -> float:\n",
    "    return levenshtein(ref.split(), hyp.split()) / max(1, len(ref.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acb74535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_beam_decoder(codec):\n",
    "    \"\"\"\n",
    "    Build a token-level CTC beam search decoder (no lexicon / no LM).\n",
    "    Returns None if torchaudio beam is unavailable.\n",
    "    \"\"\"\n",
    "    if not HAS_TORCHAUDIO_BEAM:\n",
    "        return None\n",
    "    # tokens must be index-aligned with model outputs where 0 is blank\n",
    "    tokens = [\"<blk>\"] + [codec.idx2char[i] for i in sorted(codec.idx2char)]\n",
    "    return ctc_decoder(\n",
    "        lexicon=None,        # token-level beam (no lexicon)\n",
    "        tokens=tokens,\n",
    "        beam_size=10,        # try 5–20; 10 is a good start\n",
    "    )\n",
    "\n",
    "def decode_beam_batch(log_probs, codec, beam_decoder):\n",
    "    \"\"\"\n",
    "    log_probs: (T, B, C) on CPU\n",
    "    Returns: List[str] length B\n",
    "    \"\"\"\n",
    "    results = beam_decoder(log_probs)  # list of length B; each is [best, ...]\n",
    "    hyps = []\n",
    "    for beams in results:\n",
    "        best = beams[0]\n",
    "        # best.tokens are class indices; 0 is blank. They are already CTC-collapsed.\n",
    "        try:\n",
    "            seq = [codec.idx2char[i] for i in best.tokens if i != codec.blank_idx]  # causes KeyError if idx not found, todo\n",
    "        except KeyError:\n",
    "            seq = []\n",
    "        hyps.append(''.join(seq))\n",
    "    return hyps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19444b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 7) Training / Validation\n",
    "# --------------------------\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    labels_file: str = os.path.join(\"images\", \"labels.txt\")\n",
    "    batch_size: int = 16\n",
    "    epochs: int = 30\n",
    "    lr: float = 1e-3\n",
    "    num_workers: int = 4\n",
    "    height: int = 64\n",
    "    seed: int = 42\n",
    "    aug_factor: int = 4   # original + 4× augmented = 5× total\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def make_dataloaders(cfg: TrainConfig, codec: CTCCodec):\n",
    "    # Load all samples once\n",
    "    all_data = []\n",
    "    with open(cfg.labels_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip('\\n')\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            parts = line.split('\\t', 1)\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            all_data.append(line)\n",
    "\n",
    "    train_lines, val_lines = train_test_split(all_data, test_size=0.1, random_state=cfg.seed, shuffle=True)\n",
    "\n",
    "    # Save temporary split files\n",
    "    train_file = os.path.join(\"images\", \"train_split.txt\")\n",
    "    val_file = os.path.join(\"images\", \"val_split.txt\")\n",
    "    with open(train_file, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(train_lines))\n",
    "    with open(val_file, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(val_lines))\n",
    "\n",
    "    # Create datasets\n",
    "    base_train = LinesFile(train_file, codec, target_h=cfg.height, keep_aspect=True)\n",
    "    aug_wrappers = [AugmentedWrapper(base_train) for _ in range(cfg.aug_factor)]\n",
    "    train_set = ConcatDataset([base_train] + aug_wrappers)\n",
    "\n",
    "    val_set = LinesFile(val_file, codec, target_h=cfg.height, keep_aspect=True)\n",
    "\n",
    "    collate = PadCollate(multiple_of=4, height=cfg.height)\n",
    "    train_loader = DataLoader(train_set, batch_size=cfg.batch_size, shuffle=True,\n",
    "                              num_workers=cfg.num_workers, pin_memory=True, collate_fn=collate)\n",
    "    val_loader = DataLoader(val_set, batch_size=cfg.batch_size, shuffle=False,\n",
    "                            num_workers=cfg.num_workers, pin_memory=True, collate_fn=collate)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device, codec: CTCCodec, log_interval=100):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    loader_size = len(loader)\n",
    "    for step, batch in enumerate(loader, 1):\n",
    "        imgs = batch.imgs.to(device)\n",
    "        labels = batch.labels.to(device)\n",
    "        label_lengths = batch.label_lengths.to(device)\n",
    "        input_lengths = batch.input_lengths.to(device)\n",
    "\n",
    "        logits = model(imgs)  # (T, B, C)\n",
    "        log_probs = logits.log_softmax(dim=-1)\n",
    "\n",
    "        loss = criterion(log_probs, labels, input_lengths, label_lengths)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if step % log_interval == 0:\n",
    "            avg = running_loss / log_interval\n",
    "            print(f\"  step {step:5d} / {loader_size} | train loss {avg:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "def validate(model, loader, device, codec: CTCCodec):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Greedy metrics\n",
    "    total_cer_g = 0.0\n",
    "    total_wer_g = 0.0\n",
    "\n",
    "    # Beam metrics\n",
    "    beam_decoder = build_beam_decoder(codec)\n",
    "    total_cer_b = 0.0\n",
    "    total_wer_b = 0.0\n",
    "    count = 0\n",
    "\n",
    "    criterion = nn.CTCLoss(blank=codec.blank_idx, zero_infinity=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            if batch is None:\n",
    "                continue\n",
    "            imgs = batch.imgs.to(device)\n",
    "            labels = batch.labels.to(device)\n",
    "            label_lengths = batch.label_lengths.to(device)\n",
    "            input_lengths = batch.input_lengths.to(device)\n",
    "\n",
    "            logits = model(imgs)\n",
    "            log_probs = logits.log_softmax(dim=-1)\n",
    "            loss = criterion(log_probs, labels, input_lengths, label_lengths)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Greedy decode for metrics\n",
    "            hyps_g = codec.decode_greedy(log_probs)\n",
    "            for hyp, ref in zip(hyps_g, batch.texts):\n",
    "                total_cer_g += cer(ref, hyp)\n",
    "                total_wer_g += wer(ref, hyp)\n",
    "                \n",
    "            # Beam decode\n",
    "            if beam_decoder is not None:\n",
    "                hyps_b = decode_beam_batch(log_probs.cpu(), codec, beam_decoder)\n",
    "                for hyp, ref in zip(hyps_b, batch.texts):\n",
    "                    total_cer_b += cer(ref, hyp)\n",
    "                    total_wer_b += wer(ref, hyp)\n",
    "\n",
    "            count += len(batch.texts)\n",
    "\n",
    "    # Averages\n",
    "    metrics = {\n",
    "        \"loss\": total_loss / max(1, len(loader)),\n",
    "        \"greedy\": {\n",
    "            \"cer\": total_cer_g / max(1, count),\n",
    "            \"wer\": total_wer_g / max(1, count),\n",
    "        },\n",
    "        \"beam\": None\n",
    "    }\n",
    "    if beam_decoder is not None:\n",
    "        metrics[\"beam\"] = {\n",
    "            \"cer\": total_cer_b / max(1, count),\n",
    "            \"wer\": total_wer_b / max(1, count),\n",
    "        }\n",
    "\n",
    "    # Pretty print for quick comparison\n",
    "    if metrics[\"beam\"] is not None:\n",
    "        print(f\"  Val loss: {metrics['loss']:.4f} | \"\n",
    "              f\"Greedy CER {metrics['greedy']['cer']:.4f} WER {metrics['greedy']['wer']:.4f} | \"\n",
    "              f\"Beam CER {metrics['beam']['cer']:.4f} WER {metrics['beam']['wer']:.4f}\")\n",
    "    else:\n",
    "        print(f\"  Val loss: {metrics['loss']:.4f} | \"\n",
    "              f\"Greedy CER {metrics['greedy']['cer']:.4f} WER {metrics['greedy']['wer']:.4f} \"\n",
    "              f\"(torchaudio beam not available)\")\n",
    "    return metrics\n",
    "\n",
    "def validate_with_lm(model, loader, device, codec, lm, lm_weight=0.5):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_cer = 0.0\n",
    "    total_wer = 0.0\n",
    "    count = 0\n",
    "\n",
    "    criterion = nn.CTCLoss(blank=codec.blank_idx, zero_infinity=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            if batch is None:\n",
    "                continue\n",
    "            imgs = batch.imgs.to(device)\n",
    "            labels = batch.labels.to(device)\n",
    "            label_lengths = batch.label_lengths.to(device)\n",
    "            input_lengths = batch.input_lengths.to(device)\n",
    "\n",
    "            logits = model(imgs)              # (T, B, C)\n",
    "            log_probs = logits.log_softmax(dim=-1).cpu()\n",
    "            loss = criterion(log_probs, labels, input_lengths, label_lengths)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            T, B, C = log_probs.shape\n",
    "            for b in range(B):\n",
    "                lp = log_probs[:, b, :]       # (T, C)\n",
    "                hyp = ctc_beam_search_with_char_lm(\n",
    "                    lp, codec, lm,\n",
    "                    beam_size=10,\n",
    "                    lm_weight=lm_weight,\n",
    "                    blank_idx=codec.blank_idx,\n",
    "                )\n",
    "                ref = batch.texts[b]\n",
    "\n",
    "                total_cer += cer(ref, hyp)\n",
    "                total_wer += wer(ref, hyp)\n",
    "                count += 1\n",
    "\n",
    "    metrics = {\n",
    "        \"loss\": total_loss / max(1, len(loader)),\n",
    "        \"greedy\": {\n",
    "            \"cer\": total_cer / max(1, count),\n",
    "            \"wer\": total_wer / max(1, count),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Pretty print for quick comparison\n",
    "    print(f\"  Val loss: {metrics['loss']:.4f} | \"\n",
    "          f\"Greedy CER {metrics['greedy']['cer']:.4f} WER {metrics['greedy']['wer']:.4f} \"\n",
    "          f\"(torchaudio beam not available)\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd9793ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_metrics_for_epoch(model, codec, metrics, best_val, best_cer, best_wer, best_score, cfg: TrainConfig):\n",
    "    # cer_metric = metrics['beam']['cer'] if HAS_TORCHAUDIO_BEAM else metrics['greedy']['cer']\n",
    "    # wer_metric = metrics['beam']['wer'] if HAS_TORCHAUDIO_BEAM else metrics['greedy']['wer']\n",
    "    cer_metric = metrics['greedy']['cer']\n",
    "    wer_metric = metrics['greedy']['wer']\n",
    "\n",
    "    if metrics['loss'] < best_val:\n",
    "        best_val = metrics['loss']\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            \"codec_chars\": codec.chars,\n",
    "            \"config\": cfg.__dict__,\n",
    "        }, \"best_crnn_ctc_val.pth\")\n",
    "        print(\"  Saved: best_crnn_ctc_val.pth\")\n",
    "\n",
    "    if cer_metric < best_cer:\n",
    "        best_cer = cer_metric\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            \"codec_chars\": codec.chars,\n",
    "            \"config\": cfg.__dict__,\n",
    "        }, \"best_crnn_ctc_cer.pth\")\n",
    "        print(\"  Saved: best_crnn_ctc_cer.pth\")\n",
    "\n",
    "    if wer_metric < best_wer:\n",
    "        best_wer = wer_metric\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            \"codec_chars\": codec.chars,\n",
    "            \"config\": cfg.__dict__,\n",
    "        }, \"best_crnn_ctc_wer.pth\")\n",
    "        print(\"  Saved: best_crnn_ctc_wer.pth\")\n",
    "\n",
    "    score = cer_metric + 0.5 * wer_metric\n",
    "    if score < best_score:\n",
    "        best_score = score\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            \"codec_chars\": codec.chars,\n",
    "            \"config\": cfg.__dict__,\n",
    "        }, \"best_crnn_ctc_score.pth\")\n",
    "        print(\"  Saved: best_crnn_ctc_score.pth\")\n",
    "\n",
    "    return best_val, best_cer, best_wer, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f6d6751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(lm: CharTrigramLM):\n",
    "    cfg = TrainConfig(\n",
    "        labels_file=os.path.join(\"images\", \"labels.txt\"),\n",
    "        batch_size=16,\n",
    "        epochs=100,\n",
    "        lr=5e-4,\n",
    "        num_workers=0,\n",
    "        height=64,\n",
    "        seed=42,\n",
    "        aug_factor=5,\n",
    "    )\n",
    "\n",
    "    set_seed(cfg.seed)\n",
    "\n",
    "    charset = default_charset()\n",
    "    codec = CTCCodec(charset)\n",
    "    train_loader, val_loader = make_dataloaders(cfg, codec)\n",
    "\n",
    "    num_classes = 1 + len(charset)\n",
    "    model = CRNN(num_classes=num_classes, in_channels=1).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=cfg.lr, steps_per_epoch=len(train_loader), epochs=cfg.epochs\n",
    "    )\n",
    "    criterion = nn.CTCLoss(blank=codec.blank_idx, zero_infinity=True)\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_cer = float(\"inf\")\n",
    "    best_wer = float(\"inf\")\n",
    "    best_score = float(\"inf\")\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{cfg.epochs}\")\n",
    "        train_one_epoch(model, train_loader, criterion, optimizer, device, codec, log_interval=100)\n",
    "        metrics = validate_with_lm(model, val_loader, device, codec, lm=lm, lm_weight=0.5) if lm is not None else validate(model, val_loader, device, codec)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"  Val loss: {metrics['loss']:.4f} | CER: {metrics['greedy']['cer']:.4f} | WER: {metrics['greedy']['wer']:.4f}\")\n",
    "\n",
    "        best_val, best_cer, best_wer, best_score = update_metrics_for_epoch(\n",
    "            model, codec, metrics,\n",
    "            best_val=best_val,\n",
    "            best_cer=best_cer,\n",
    "            best_wer=best_wer,\n",
    "            best_score=best_score,\n",
    "            cfg=cfg\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a3327b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESUME_PATH = \"best_crnn_ctc_100_epochs_0_4156_CER_0_8452_WER.pth\"\n",
    "TARGET_EPOCHS = 100\n",
    "\n",
    "def resume_training(lm: CharTrigramLM):\n",
    "    set_seed(42)\n",
    "\n",
    "    # 1) Rebuild codec/model from checkpoint\n",
    "    ckpt = torch.load(RESUME_PATH, map_location=\"cpu\")\n",
    "    codec = CTCCodec(ckpt[\"codec_chars\"][1:])  # strip <BLK> placeholder\n",
    "\n",
    "    # 2) Re-create config & loaders (same labels.txt split logic as before)\n",
    "    cfg = TrainConfig(\n",
    "        labels_file=os.path.join(\"images\", \"labels.txt\"),\n",
    "        batch_size=16,\n",
    "        epochs=TARGET_EPOCHS,  # we'll run until 100\n",
    "        lr=5e-4,               # smaller LR for continued training\n",
    "        num_workers=0,\n",
    "        height=64,\n",
    "        seed=42,\n",
    "        aug_factor=5,\n",
    "    )\n",
    "    train_loader, val_loader = make_dataloaders(cfg, codec)\n",
    "\n",
    "    # 3) Rebuild model and load weights\n",
    "    num_classes = len(ckpt[\"codec_chars\"])\n",
    "    model = CRNN(num_classes=num_classes, in_channels=1).to(device)\n",
    "    model.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "\n",
    "    # 4) Fresh optimizer/scheduler for the new phase\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=1e-4)\n",
    "    # Simple scheduler that’s easy to resume mid-run:\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader)* (TARGET_EPOCHS//5 + 1))\n",
    "    criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_cer = float(\"inf\")\n",
    "    best_wer = float(\"inf\")\n",
    "    best_score = float(\"inf\")\n",
    "    # If you remember where you stopped (say epoch=30), you can set start_epoch=31.\n",
    "    # Otherwise just keep going; training is robust to extra epochs with low LR.\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        print(f\"\\n[Resume Phase] Epoch {epoch}/{cfg.epochs}\")\n",
    "        train_one_epoch(model, train_loader, criterion, optimizer, device, codec, log_interval=100)\n",
    "        # metrics = validate(model, val_loader, device, codec)\n",
    "        metrics = validate_with_lm(model, val_loader, device, codec, lm=lm, lm_weight=0.5)\n",
    "        scheduler.step()\n",
    "\n",
    "        best_val, best_cer, best_wer, best_score = update_metrics_for_epoch(\n",
    "            model, codec, metrics,\n",
    "            best_val=best_val,\n",
    "            best_cer=best_cer,\n",
    "            best_wer=best_wer,\n",
    "            best_score=best_score,\n",
    "            cfg=cfg\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3404de85",
   "metadata": {},
   "source": [
    "**Inference code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2be2843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Load model checkpoint ----------\n",
    "def load_model(checkpoint_path, device):\n",
    "    ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "    codec = CTCCodec(ckpt[\"codec_chars\"][1:])  # skip <BLK> token\n",
    "    model = CRNN(num_classes=len(ckpt[\"codec_chars\"]), in_channels=1).to(device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    model.eval()\n",
    "    return model, codec\n",
    "\n",
    "# ---------- Preprocess single image ----------\n",
    "def preprocess_image(img, target_h=64):\n",
    "    # img = Image.open(img_path).convert(\"L\")\n",
    "    img = ImageOps.exif_transpose(img)\n",
    "    resize = KeepRatioResize(target_h)\n",
    "    img = resize(img)\n",
    "    tensor = pil_to_tensor_normalized(img).unsqueeze(0)  # (1,1,H,W)\n",
    "    return tensor, img.size  # (W,H)\n",
    "\n",
    "# ---------- Decode prediction ----------\n",
    "def predict(model, codec, img_tensor, device):\n",
    "    with torch.no_grad():\n",
    "        img_tensor = img_tensor.to(device)\n",
    "        logits = model(img_tensor)         # (T, B, C)\n",
    "        log_probs = logits.log_softmax(dim=-1)\n",
    "        if HAS_TORCHAUDIO_BEAM:\n",
    "            beam_decoder = build_beam_decoder(codec)\n",
    "            text = decode_beam_batch(log_probs.cpu(), codec, beam_decoder)[0]\n",
    "        else:\n",
    "            text = codec.decode_greedy(log_probs)[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d81d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antpa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step   100 | train loss 8.5171\n",
      "  step   200 | train loss 3.3493\n",
      "  step   300 | train loss 3.2249\n",
      "  step   400 | train loss 3.2065\n",
      "  step   500 | train loss 3.1891\n",
      "  step   600 | train loss 3.1835\n",
      "  step   700 | train loss 3.1848\n",
      "  step   800 | train loss 3.1810\n",
      "  step   900 | train loss 3.1888\n",
      "  step  1000 | train loss 3.1738\n",
      "  step  1100 | train loss 3.1770\n",
      "  step  1200 | train loss 3.1725\n",
      "  step  1300 | train loss 3.1613\n",
      "  step  1400 | train loss 3.1797\n",
      "  step  1500 | train loss 3.1643\n",
      "  step  1600 | train loss 3.1506\n",
      "  step  1700 | train loss 3.1530\n",
      "  step  1800 | train loss 3.1669\n",
      "  step  1900 | train loss 3.1569\n",
      "  step  2000 | train loss 3.1545\n",
      "  step  2100 | train loss 3.1485\n",
      "  step  2200 | train loss 3.1435\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # # Load a character-level trigram LM for training with LM-based validation\n",
    "    # texts = load_train_texts_from_labels(os.path.join(\"images\", \"labels.txt\"))\n",
    "    # lm = CharTrigramLM(alpha=0.1)\n",
    "    # lm.train(texts)\n",
    "\n",
    "    # train_loop(lm)\n",
    "    train_loop(None)\n",
    "\n",
    "    # resume_training(lm)\n",
    "\n",
    "    # # Inference\n",
    "    # model_path = \"best_crnn_ctc_64_epochs_0_0324_CER_0_1349_WER.pth\"\n",
    "    # # model_path = \"best_crnn_ctc_53_epochs_0_0391_CER_0_1841_WER.pth\"\n",
    "\n",
    "    # print(f\"Loading model from {model_path} ...\")\n",
    "    # model, codec = load_model(model_path, device)\n",
    "\n",
    "    # # Calculate and print the total number of trainable parameters\n",
    "    # total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    # print(f\"Total number of trainable parameters: {total_params}\")\n",
    "\n",
    "    # # image_path = os.path.join(\"sample_images\", \"0008_0.jpg\")\n",
    "    # # image_path = os.path.join(\"sample_images\", \"sample_image.jpg\")\n",
    "    # # image_path = os.path.join(\"sample_images\", \"20251103_061143.jpg\")\n",
    "    # # image_path = os.path.join(\"lines_strict\", \"a4_handwriting_line_00.jpg\")\n",
    "    # image_path = \"a4_handwriting.png\"\n",
    "    # print(f\"Running inference on {image_path} ...\")\n",
    "    \n",
    "    # lines = get_lines(\"a4_handwriting.png\")\n",
    "    # pred_text = ''\n",
    "    # for i in range(len(lines)):\n",
    "    #     print(f\"Processing line {i+1}/{len(lines)}\")\n",
    "    #     img_tensor, (w, h) = preprocess_image(Image.fromarray(lines[i]), target_h=64)\n",
    "    #     pred_text += predict(model, codec, img_tensor, device)\n",
    "\n",
    "    # print(f\"\\nPredicted text:\\n\\\"{pred_text}\\\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
