{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3305d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import string\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f115a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 1) Character set & CTC codec\n",
    "# ----------------------------\n",
    "def default_charset():\n",
    "    # You can customize this to match your data (e.g., only lowercase + space)\n",
    "    # Keep space ' ' included if your lines contain spaces.\n",
    "    charset = list(string.digits + string.ascii_letters + string.punctuation + ' ')\n",
    "    # Remove characters you know you don't have, or add accents if needed.\n",
    "    return charset\n",
    "\n",
    "class CTCCodec:\n",
    "    \"\"\"\n",
    "    Maps characters <-> indices. Index 0 is reserved for CTC blank.\n",
    "    \"\"\"\n",
    "    def __init__(self, charset: List[str]):\n",
    "        self.blank_idx = 0\n",
    "        self.chars = ['<BLK>'] + charset\n",
    "        self.char2idx = {c: i+1 for i, c in enumerate(charset)}  # shift by +1\n",
    "        self.idx2char = {i+1: c for i, c in enumerate(charset)}\n",
    "\n",
    "    def encode(self, text: str) -> torch.Tensor:\n",
    "        return torch.tensor([self.char2idx[c] for c in text if c in self.char2idx], dtype=torch.long)\n",
    "\n",
    "    def decode_greedy(self, logits: torch.Tensor) -> List[str]:\n",
    "        \"\"\"\n",
    "        logits: (T, N, C) log-probs or raw scores. We'll argmax over classes.\n",
    "        Returns list of length N with collapsed CTC decoding.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(dim=-1)  # (T, N)\n",
    "            pred = pred.cpu().numpy()\n",
    "        N = pred.shape[1]\n",
    "        texts = []\n",
    "        for n in range(N):\n",
    "            seq = pred[:, n]\n",
    "            prev = -1\n",
    "            out = []\n",
    "            for idx in seq:\n",
    "                if idx != self.blank_idx and idx != prev:\n",
    "                    out.append(self.idx2char.get(int(idx), ''))\n",
    "                prev = idx\n",
    "            texts.append(''.join(out))\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caf5962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# 2) Image transforms & augmentations\n",
    "# ----------------------------------\n",
    "class KeepRatioResize:\n",
    "    \"\"\"\n",
    "    Resize PIL image to target height with proportional width, no crop.\n",
    "    \"\"\"\n",
    "    def __init__(self, target_h: int):\n",
    "        self.target_h = target_h\n",
    "\n",
    "    def __call__(self, img: Image.Image) -> Image.Image:\n",
    "        w, h = img.size\n",
    "        if h == self.target_h:\n",
    "            return img\n",
    "        new_w = max(1, round(w * (self.target_h / h)))\n",
    "        return img.resize((new_w, self.target_h), Image.BILINEAR)\n",
    "\n",
    "class ElasticLike:\n",
    "    \"\"\"\n",
    "    Lightweight 'elastic' style warp using PIL perspective + slight blur/sharpen.\n",
    "    Keeps text legible but varied.\n",
    "    \"\"\"\n",
    "    def __init__(self, p=0.5, max_warp=0.08):\n",
    "        self.p = p\n",
    "        self.max_warp = max_warp\n",
    "\n",
    "    def __call__(self, img: Image.Image) -> Image.Image:\n",
    "        if random.random() > self.p:\n",
    "            return img\n",
    "        w, h = img.size\n",
    "        dx = int(self.max_warp * w)\n",
    "        dy = int(self.max_warp * h)\n",
    "        # random offsets for corners\n",
    "        src = [(0,0),(w,0),(w,h),(0,h)]\n",
    "        img = img.transform((w + random.randint(-dx, dx), h + random.randint(-dx, dx)), Image.QUAD, src)\n",
    "        if random.random() < 0.5:\n",
    "            img = img.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.2, 0.6)))\n",
    "        if random.random() < 0.3:\n",
    "            img = img.filter(ImageFilter.UnsharpMask(radius=1.0, percent=80, threshold=3))\n",
    "        return img\n",
    "\n",
    "def pil_to_tensor_normalized(img: Image.Image) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert PIL (grayscale) -> Tensor in [0,1], normalize to mean=0.5, std=0.5\n",
    "    Output shape: (1, H, W)\n",
    "    \"\"\"\n",
    "    t = transforms.functional.pil_to_tensor(img).float() / 255.0  # (1,H,W) for 'L'\n",
    "    return transforms.functional.normalize(t, mean=[0.5], std=[0.5])\n",
    "\n",
    "def binarize_if_needed(img: Image.Image, p=0.0):\n",
    "    if p > 0 and random.random() < p:\n",
    "        return img.convert('L').point(lambda x: 255 if x > 200 else 0, mode='L')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37191bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# 3) Dataset definitions\n",
    "# ------------------------\n",
    "class LinesFile(Dataset):\n",
    "    \"\"\"\n",
    "    labels.txt format: path<TAB>text (UTF-8)\n",
    "    Converts to grayscale, resizes to H=64 with proportional width.\n",
    "    \"\"\"\n",
    "    def __init__(self, labels_file: str, codec: CTCCodec, target_h: int = 64, keep_aspect=True, binarize_p=0.0):\n",
    "        super().__init__()\n",
    "        self.samples = []\n",
    "        self.folder_path = os.path.dirname(labels_file)\n",
    "        with open(labels_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.rstrip('\\n')\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                parts = line.split('\\t', 1)\n",
    "                if len(parts) != 2:\n",
    "                    continue\n",
    "                path, text = parts\n",
    "                self.samples.append((path, text))\n",
    "        self.codec = codec\n",
    "        self.target_h = target_h\n",
    "        self.keep_aspect = keep_aspect\n",
    "        self.resize = KeepRatioResize(target_h)\n",
    "        self.binarize_p = binarize_p\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file, text = self.samples[idx]\n",
    "        path = os.path.join(self.folder_path, file)\n",
    "        img = Image.open(path).convert('L')\n",
    "        img = ImageOps.exif_transpose(img)\n",
    "        if self.keep_aspect:\n",
    "            img = self.resize(img)\n",
    "        img = binarize_if_needed(img, self.binarize_p)\n",
    "        tensor = pil_to_tensor_normalized(img)\n",
    "        label = self.codec.encode(text)\n",
    "        return tensor, label, text, os.path.basename(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "602dba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedWrapper(Dataset):\n",
    "    \"\"\"\n",
    "    Wrap a base dataset and apply strong augmentations.\n",
    "    Use multiple instances of this wrapper to grow dataset size to >=5x.\n",
    "    \"\"\"\n",
    "    def __init__(self, base: LinesFile):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        # Compose handwriting-friendly augmentations\n",
    "        self.resize = KeepRatioResize(base.target_h)\n",
    "        self.aug = transforms.RandomChoice([\n",
    "            transforms.RandomAffine(degrees=2, translate=(0.02, 0.03), scale=(0.95, 1.05), shear=(-2, 2), fill=255),\n",
    "            transforms.RandomPerspective(distortion_scale=0.3, p=1.0),\n",
    "        ])\n",
    "        self.colorjitter = transforms.ColorJitter(brightness=0.2, contrast=0.2)\n",
    "        self.elastic = ElasticLike(p=0.7, max_warp=0.06)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tensor, label, text, name = self.base[idx]\n",
    "        # back to PIL to apply augmentations that expect PIL\n",
    "        H = tensor.shape[1]\n",
    "        W = tensor.shape[2]\n",
    "        pil = transforms.functional.to_pil_image(((tensor * 0.5 + 0.5) * 255.0).byte())  # unnormalize for aug\n",
    "        # augment in PIL space\n",
    "        pil = self.elastic(pil)\n",
    "        pil = self.aug(pil)\n",
    "        pil = self.colorjitter(pil)\n",
    "        # Small random Gaussian blur helps mimic scanning\n",
    "        if random.random() < 0.3:\n",
    "            pil = pil.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.2, 0.7)))\n",
    "        # Ensure size back to desired height (keeps ratio)\n",
    "        pil = self.resize(pil)\n",
    "        # Occasionally invert (handwriting scans vary)\n",
    "        if random.random() < 0.25:\n",
    "            pil = ImageOps.invert(pil)\n",
    "        tensor_aug = pil_to_tensor_normalized(pil)\n",
    "        return tensor_aug, label, text, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24c50a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# 4) Collate: pad widths & build lengths\n",
    "# ---------------------------------------\n",
    "@dataclass\n",
    "class Batch:\n",
    "    imgs: torch.Tensor        # (B, 1, H, Wmax)\n",
    "    labels: torch.Tensor      # (sum_targets,)\n",
    "    label_lengths: torch.Tensor  # (B,)\n",
    "    input_lengths: torch.Tensor  # (B,)  number of time steps per sample after CNN\n",
    "    texts: List[str]\n",
    "    names: List[str]\n",
    "    orig_widths: List[int]\n",
    "\n",
    "class PadCollate:\n",
    "    \"\"\"\n",
    "    Pads each batch to max width (also to multiple of 4) and prepares CTC lengths.\n",
    "    \"\"\"\n",
    "    def __init__(self, multiple_of: int = 4, height: int = 64):\n",
    "        self.multiple_of = multiple_of\n",
    "        self.height = height\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # batch: list of (tensor(1,H,W), label, text, name)\n",
    "        imgs, labels, texts, names, widths = [], [], [], [], []\n",
    "        for t, lab, txt, name in batch:\n",
    "            _, h, w = t.size()\n",
    "            assert h == self.height, f\"Expected height {self.height}, got {h}\"\n",
    "            imgs.append(t)\n",
    "            labels.append(lab)\n",
    "            texts.append(txt)\n",
    "            names.append(name)\n",
    "            widths.append(w)\n",
    "\n",
    "        B = len(imgs)\n",
    "        max_w = max(widths)\n",
    "        # pad to next multiple of self.multiple_of (for CNN width downsampling)\n",
    "        if self.multiple_of > 1:\n",
    "            max_w = int(math.ceil(max_w / self.multiple_of) * self.multiple_of)\n",
    "\n",
    "        padded = torch.full((B, 1, self.height, max_w), fill_value=(0.5 - 0.5)/0.5, dtype=imgs[0].dtype)\n",
    "        # Explanation: because we normalized to mean=0.5, std=0.5,\n",
    "        # \"white\" (1.0) becomes (1-0.5)/0.5 = +1.0, \"gray 0.5\" is 0; but to avoid halo,\n",
    "        # we can pad with normalized value of 1.0 (white) -> +1.0:\n",
    "        padded.fill_(+1.0)\n",
    "\n",
    "        for i, t in enumerate(imgs):\n",
    "            _, _, w = t.size()\n",
    "            padded[i, :, :, :w] = t\n",
    "\n",
    "        labels_concat = torch.cat(labels, dim=0)\n",
    "        label_lengths = torch.tensor([len(l) for l in labels], dtype=torch.long)\n",
    "\n",
    "        # We'll use a CNN that downsamples width by factor 4 -> input_lengths = ceil(w/4)\n",
    "        input_lengths = torch.tensor([math.ceil(w / 4) for w in widths], dtype=torch.long)\n",
    "\n",
    "        return Batch(\n",
    "            imgs=padded,\n",
    "            labels=labels_concat,\n",
    "            label_lengths=label_lengths,\n",
    "            input_lengths=input_lengths,\n",
    "            texts=texts,\n",
    "            names=names,\n",
    "            orig_widths=widths\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2b01884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 5) CRNN model (CNN + BiLSTM)\n",
    "# ---------------------------\n",
    "class CRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN reduces H and W (width by 4x overall), then we pool height to 1 and treat width as time.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes: int, in_channels=1):\n",
    "        super().__init__()\n",
    "        # VGG-ish feature extractor; keep it simple and efficient\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # H/2, W/2\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # H/4, W/4\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            # keep width stride=1 here\n",
    "\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(inplace=True),\n",
    "        )\n",
    "        # Collapse height to 1 with adaptive pooling; width stays ~W/4 due to the two pools above.\n",
    "        self.height_pool = nn.AdaptiveAvgPool2d((1, None))\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=256, hidden_size=256, num_layers=2,\n",
    "            bidirectional=True, dropout=0.1, batch_first=False\n",
    "        )\n",
    "        self.fc = nn.Linear(512, num_classes)  # 2*hidden\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B,1,H,W)\n",
    "        f = self.features(x)                        # (B, C=256, H', W')\n",
    "        f = self.height_pool(f).squeeze(2)          # (B, C, W')\n",
    "        f = f.permute(2, 0, 1)                      # (T=W', B, C)\n",
    "        out, _ = self.rnn(f)                        # (T, B, 2*H)\n",
    "        logits = self.fc(out)                       # (T, B, num_classes)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e414e52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 6) Metrics: CER / WER\n",
    "# --------------------------\n",
    "def levenshtein(a: List[str], b: List[str]) -> int:\n",
    "    # Levenshtein distance for lists of tokens (chars or words)\n",
    "    dp = [[0]*(len(b)+1) for _ in range(len(a)+1)]\n",
    "    for i in range(len(a)+1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len(b)+1):\n",
    "        dp[0][j] = j\n",
    "    for i in range(1, len(a)+1):\n",
    "        for j in range(1, len(b)+1):\n",
    "            cost = 0 if a[i-1]==b[j-1] else 1\n",
    "            dp[i][j] = min(dp[i-1][j]+1, dp[i][j-1]+1, dp[i-1][j-1]+cost)\n",
    "    return dp[-1][-1]\n",
    "\n",
    "def cer(ref: str, hyp: str) -> float:\n",
    "    return levenshtein(list(ref), list(hyp)) / max(1, len(ref))\n",
    "\n",
    "def wer(ref: str, hyp: str) -> float:\n",
    "    return levenshtein(ref.split(), hyp.split()) / max(1, len(ref.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e19444b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 7) Training / Validation\n",
    "# --------------------------\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    labels_file: str = os.path.join(\"images\", \"labels.txt\")\n",
    "    batch_size: int = 16\n",
    "    epochs: int = 30\n",
    "    lr: float = 1e-3\n",
    "    num_workers: int = 4\n",
    "    height: int = 64\n",
    "    seed: int = 42\n",
    "    aug_factor: int = 4   # original + 4× augmented = 5× total\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def make_dataloaders(cfg: TrainConfig, codec: CTCCodec):\n",
    "    # Load all samples once\n",
    "    all_data = []\n",
    "    with open(cfg.labels_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip('\\n')\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            parts = line.split('\\t', 1)\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            all_data.append(line)\n",
    "\n",
    "    train_lines, val_lines = train_test_split(all_data, test_size=0.1, random_state=cfg.seed, shuffle=True)\n",
    "\n",
    "    # Save temporary split files\n",
    "    train_file = os.path.join(\"images\", \"train_split.txt\")\n",
    "    val_file = os.path.join(\"images\", \"val_split.txt\")\n",
    "    with open(train_file, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(train_lines))\n",
    "    with open(val_file, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(val_lines))\n",
    "\n",
    "    # Create datasets\n",
    "    base_train = LinesFile(train_file, codec, target_h=cfg.height, keep_aspect=True)\n",
    "    aug_wrappers = [AugmentedWrapper(base_train) for _ in range(cfg.aug_factor)]\n",
    "    train_set = ConcatDataset([base_train] + aug_wrappers)\n",
    "\n",
    "    val_set = LinesFile(val_file, codec, target_h=cfg.height, keep_aspect=True)\n",
    "\n",
    "    collate = PadCollate(multiple_of=4, height=cfg.height)\n",
    "    train_loader = DataLoader(train_set, batch_size=cfg.batch_size, shuffle=True,\n",
    "                              num_workers=cfg.num_workers, pin_memory=True, collate_fn=collate)\n",
    "    val_loader = DataLoader(val_set, batch_size=cfg.batch_size, shuffle=False,\n",
    "                            num_workers=cfg.num_workers, pin_memory=True, collate_fn=collate)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device, codec: CTCCodec, log_interval=100):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for step, batch in enumerate(loader, 1):\n",
    "        imgs = batch.imgs.to(device)\n",
    "        labels = batch.labels.to(device)\n",
    "        label_lengths = batch.label_lengths.to(device)\n",
    "        input_lengths = batch.input_lengths.to(device)\n",
    "\n",
    "        logits = model(imgs)  # (T, B, C)\n",
    "        log_probs = logits.log_softmax(dim=-1)\n",
    "\n",
    "        loss = criterion(log_probs, labels, input_lengths, label_lengths)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if step % log_interval == 0:\n",
    "            avg = running_loss / log_interval\n",
    "            print(f\"  step {step:5d} | train loss {avg:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "def validate(model, loader, device, codec: CTCCodec):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_cer = 0.0\n",
    "    total_wer = 0.0\n",
    "    count = 0\n",
    "    criterion = nn.CTCLoss(blank=codec.blank_idx, zero_infinity=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            imgs = batch.imgs.to(device)\n",
    "            labels = batch.labels.to(device)\n",
    "            label_lengths = batch.label_lengths.to(device)\n",
    "            input_lengths = batch.input_lengths.to(device)\n",
    "\n",
    "            logits = model(imgs)\n",
    "            log_probs = logits.log_softmax(dim=-1)\n",
    "            loss = criterion(log_probs, labels, input_lengths, label_lengths)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Greedy decode for metrics\n",
    "            hyps = codec.decode_greedy(log_probs)\n",
    "            for hyp, ref in zip(hyps, batch.texts):\n",
    "                total_cer += cer(ref, hyp)\n",
    "                total_wer += wer(ref, hyp)\n",
    "                count += 1\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / max(1, len(loader)),\n",
    "        \"cer\": total_cer / max(1, count),\n",
    "        \"wer\": total_wer / max(1, count),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f6d6751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antpa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     45\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  Saved checkpoint: best_crnn_ctc.pth\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, cfg.epochs + \u001b[32m1\u001b[39m):\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg.epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     metrics = validate(model, val_loader, device, codec)\n\u001b[32m     34\u001b[39m     scheduler.step()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, criterion, optimizer, device, codec, log_interval)\u001b[39m\n\u001b[32m     58\u001b[39m model.train()\n\u001b[32m     59\u001b[39m running_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataset.py:346\u001b[39m, in \u001b[36mConcatDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    345\u001b[39m     sample_idx = idx - \u001b[38;5;28mself\u001b[39m.cumulative_sizes[dataset_idx - \u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_idx\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mAugmentedWrapper.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     26\u001b[39m pil = transforms.functional.to_pil_image(((tensor * \u001b[32m0.5\u001b[39m + \u001b[32m0.5\u001b[39m) * \u001b[32m255.0\u001b[39m).byte())  \u001b[38;5;66;03m# unnormalize for aug\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# augment in PIL space\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m pil = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43melastic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpil\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m pil = \u001b[38;5;28mself\u001b[39m.aug(pil)\n\u001b[32m     30\u001b[39m pil = \u001b[38;5;28mself\u001b[39m.colorjitter(pil)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mElasticLike.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# random offsets for corners\u001b[39;00m\n\u001b[32m     34\u001b[39m src = [(\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m),(w,\u001b[32m0\u001b[39m),(w,h),(\u001b[32m0\u001b[39m,h)]\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m img = \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[43mdx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[43mdx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mQUAD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m random.random() < \u001b[32m0.5\u001b[39m:\n\u001b[32m     37\u001b[39m     img = img.filter(ImageFilter.GaussianBlur(radius=random.uniform(\u001b[32m0.2\u001b[39m, \u001b[32m0.6\u001b[39m)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\PIL\\Image.py:2880\u001b[39m, in \u001b[36mImage.transform\u001b[39m\u001b[34m(self, size, method, data, resample, fill, fillcolor)\u001b[39m\n\u001b[32m   2876\u001b[39m         im.__transformer(\n\u001b[32m   2877\u001b[39m             box, \u001b[38;5;28mself\u001b[39m, Transform.QUAD, quad, resample, fillcolor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2878\u001b[39m         )\n\u001b[32m   2879\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2880\u001b[39m     \u001b[43mim\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__transformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2881\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfillcolor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m   2882\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2884\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m im\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\PIL\\Image.py:2924\u001b[39m, in \u001b[36mImage.__transformer\u001b[39m\u001b[34m(self, box, image, method, data, resample, fill)\u001b[39m\n\u001b[32m   2920\u001b[39m     As = \u001b[32m1.0\u001b[39m / w\n\u001b[32m   2921\u001b[39m     At = \u001b[32m1.0\u001b[39m / h\n\u001b[32m   2922\u001b[39m     data = (\n\u001b[32m   2923\u001b[39m         x0,\n\u001b[32m-> \u001b[39m\u001b[32m2924\u001b[39m         (\u001b[43mne\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m - x0) * As,\n\u001b[32m   2925\u001b[39m         (sw[\u001b[32m0\u001b[39m] - x0) * At,\n\u001b[32m   2926\u001b[39m         (se[\u001b[32m0\u001b[39m] - sw[\u001b[32m0\u001b[39m] - ne[\u001b[32m0\u001b[39m] + x0) * As * At,\n\u001b[32m   2927\u001b[39m         y0,\n\u001b[32m   2928\u001b[39m         (ne[\u001b[32m1\u001b[39m] - y0) * As,\n\u001b[32m   2929\u001b[39m         (sw[\u001b[32m1\u001b[39m] - y0) * At,\n\u001b[32m   2930\u001b[39m         (se[\u001b[32m1\u001b[39m] - sw[\u001b[32m1\u001b[39m] - ne[\u001b[32m1\u001b[39m] + y0) * As * At,\n\u001b[32m   2931\u001b[39m     )\n\u001b[32m   2933\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2934\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33munknown transformation method\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    cfg = TrainConfig(\n",
    "        labels_file=os.path.join(\"images\", \"labels.txt\"),\n",
    "        batch_size=16,\n",
    "        epochs=30,\n",
    "        lr=1e-3,\n",
    "        num_workers=0,\n",
    "        height=64,\n",
    "        seed=42,\n",
    "        aug_factor=5,\n",
    "    )\n",
    "\n",
    "    set_seed(cfg.seed)\n",
    "\n",
    "    charset = default_charset()\n",
    "    codec = CTCCodec(charset)\n",
    "    train_loader, val_loader = make_dataloaders(cfg, codec)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    num_classes = 1 + len(charset)\n",
    "    model = CRNN(num_classes=num_classes, in_channels=1).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=cfg.lr, steps_per_epoch=len(train_loader), epochs=cfg.epochs\n",
    "    )\n",
    "    criterion = nn.CTCLoss(blank=codec.blank_idx, zero_infinity=True)\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{cfg.epochs}\")\n",
    "        train_one_epoch(model, train_loader, criterion, optimizer, device, codec, log_interval=100)\n",
    "        metrics = validate(model, val_loader, device, codec)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"  Val loss: {metrics['loss']:.4f} | CER: {metrics['cer']:.4f} | WER: {metrics['wer']:.4f}\")\n",
    "\n",
    "        if metrics['loss'] < best_val:\n",
    "            best_val = metrics['loss']\n",
    "            torch.save({\n",
    "                \"model\": model.state_dict(),\n",
    "                \"codec_chars\": codec.chars,\n",
    "                \"config\": cfg.__dict__,\n",
    "            }, \"best_crnn_ctc.pth\")\n",
    "            print(\"  Saved checkpoint: best_crnn_ctc.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3404de85",
   "metadata": {},
   "source": [
    "**Inference code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2be2843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Load model checkpoint ----------\n",
    "def load_model(checkpoint_path, device):\n",
    "    ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "    codec = CTCCodec(ckpt[\"codec_chars\"][1:])  # skip <BLK> token\n",
    "    model = CRNN(num_classes=len(ckpt[\"codec_chars\"]), in_channels=1).to(device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    model.eval()\n",
    "    return model, codec\n",
    "\n",
    "# ---------- Preprocess single image ----------\n",
    "def preprocess_image(img_path, target_h=64):\n",
    "    img = Image.open(img_path).convert(\"L\")\n",
    "    img = ImageOps.exif_transpose(img)\n",
    "    resize = KeepRatioResize(target_h)\n",
    "    img = resize(img)\n",
    "    tensor = pil_to_tensor_normalized(img).unsqueeze(0)  # (1,1,H,W)\n",
    "    return tensor, img.size  # (W,H)\n",
    "\n",
    "# ---------- Decode prediction ----------\n",
    "def predict(model, codec, img_tensor, device):\n",
    "    with torch.no_grad():\n",
    "        img_tensor = img_tensor.to(device)\n",
    "        logits = model(img_tensor)         # (T, B, C)\n",
    "        log_probs = logits.log_softmax(dim=-1)\n",
    "        text = codec.decode_greedy(log_probs)[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50d81d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from best_crnn_ctc.pth ...\n",
      "Running inference on 20250625_095530.jpg ...\n",
      "\n",
      "Predicted text:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_path = \"best_crnn_ctc.pth\"\n",
    "test_image = \"20250625_095530.jpg\"\n",
    "\n",
    "print(f\"Loading model from {model_path} ...\")\n",
    "model, codec = load_model(model_path, device)\n",
    "\n",
    "print(f\"Running inference on {test_image} ...\")\n",
    "img_tensor, (w, h) = preprocess_image(test_image, target_h=64)\n",
    "pred_text = predict(model, codec, img_tensor, device)\n",
    "\n",
    "print(f\"\\nPredicted text:\\n{pred_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
